{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 12682267,
          "sourceType": "datasetVersion",
          "datasetId": 8014648
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from albumentations import Compose, Resize, HorizontalFlip, RandomBrightnessContrast, GaussNoise\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from albumentations import Compose, Resize, HorizontalFlip, RandomBrightnessContrast\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T22:39:13.251556Z",
          "iopub.execute_input": "2025-08-06T22:39:13.251830Z",
          "iopub.status.idle": "2025-08-06T22:39:28.174402Z",
          "shell.execute_reply.started": "2025-08-06T22:39:13.251798Z",
          "shell.execute_reply": "2025-08-06T22:39:28.173473Z"
        },
        "id": "8HMLufzQDo9H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring structure and nomenclature"
      ],
      "metadata": {
        "id": "feNIrzXnDo9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = \"/kaggle/input/shop-dataset/Shop DataSet\"\n",
        "\n",
        "classes = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))])\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "\n",
        "def count_videos_per_class(root):\n",
        "    counter = {}\n",
        "    for cls in classes:\n",
        "        cls_path = os.path.join(root, cls)\n",
        "        vids = [f for f in os.listdir(cls_path) if f.lower().endswith(('.mp4', '.avi'))]\n",
        "        counter[cls] = len(vids)\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar(counter.keys(), counter.values())\n",
        "    plt.ylabel('Number of Videos')\n",
        "    plt.title('Video Distribution per Class')\n",
        "    plt.show()\n",
        "\n",
        "    return counter\n",
        "\n",
        "print(count_videos_per_class(data_root))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T22:39:28.175886Z",
          "iopub.execute_input": "2025-08-06T22:39:28.176527Z",
          "iopub.status.idle": "2025-08-06T22:39:28.225664Z",
          "shell.execute_reply.started": "2025-08-06T22:39:28.176502Z",
          "shell.execute_reply": "2025-08-06T22:39:28.224972Z"
        },
        "id": "5gIhm4S0Do9L",
        "outputId": "a7ea3567-36bc-42fa-9a7c-621c8d8a1440"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Classes: ['non shop lifters', 'shop lifters']\n{'non shop lifters': 531, 'shop lifters': 324}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        " Preprocessing & Augmentation"
      ],
      "metadata": {
        "id": "5NMeS8y1Do9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of preparing a spatial transform for each frame\n",
        "spatial_transform = Compose([\n",
        "    Resize(112, 112),\n",
        "    HorizontalFlip(p=0.5),\n",
        "    RandomBrightnessContrast(p=0.3),\n",
        "\n",
        "], p=1.0)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T22:39:28.226451Z",
          "iopub.execute_input": "2025-08-06T22:39:28.226731Z",
          "iopub.status.idle": "2025-08-06T22:39:28.234752Z",
          "shell.execute_reply.started": "2025-08-06T22:39:28.226700Z",
          "shell.execute_reply": "2025-08-06T22:39:28.234033Z"
        },
        "id": "Zx82kpdXDo9N"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset class with sampling and length standardization"
      ],
      "metadata": {
        "id": "JKMRVQOXDo9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TheftVideoDataset(Dataset):\n",
        "    def __init__(self, video_paths, labels, num_frames=16, transform=None, mode=None):\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.num_frames = num_frames\n",
        "        self.transform = transform  # spatial transform per frame\n",
        "        self.mode = mode  # \"train\", \"val\", \"test\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def _load_video(self, path):\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frames = []\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    def _sample_frames(self, frames):\n",
        "        total = len(frames)\n",
        "        if total >= self.num_frames:\n",
        "            if self.mode == \"train\":\n",
        "                # Random consecutive sampling\n",
        "                start = np.random.randint(0, total - self.num_frames + 1)\n",
        "                sampled = frames[start:start + self.num_frames]\n",
        "            else:\n",
        "                # Uniform sampling\n",
        "                indices = np.linspace(0, total - 1, self.num_frames, dtype=int)\n",
        "                sampled = [frames[i] for i in indices]\n",
        "        else:\n",
        "            pad = [frames[-1]] * (self.num_frames - total)\n",
        "            sampled = frames + pad\n",
        "        return sampled\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        frames = self._load_video(path)\n",
        "\n",
        "        if len(frames) == 0:\n",
        "            raise RuntimeError(f\"Failed to load video: {path}\")\n",
        "\n",
        "        # Apply temporal dropout only during training\n",
        "        if self.mode == \"train\":\n",
        "            frames = self._temporal_dropout(frames, drop_prob=0.2)\n",
        "\n",
        "        frames = self._sample_frames(frames)  # list of HxWxC\n",
        "\n",
        "        processed = []\n",
        "        for f in frames:\n",
        "            if self.transform:\n",
        "                augmented = self.transform(image=f)\n",
        "                f_proc = augmented[\"image\"]  # tensor CxHxW if ToTensorV2 used\n",
        "            else:\n",
        "                f_resized = cv2.resize(f, (112, 112))\n",
        "                f_proc = torch.from_numpy(f_resized).permute(2,0,1).float() / 255.0\n",
        "            processed.append(f_proc)\n",
        "\n",
        "        video_tensor = torch.stack(processed)  # shape: (T, C, H, W)\n",
        "        # video_tensor = video_tensor.permute(1,0,2,3)  # (C, T, H, W) for 3D CNN consistency -> Give error for any model other than 3D CNN\n",
        "        return video_tensor, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    def _temporal_dropout(self, frames, drop_prob=0.2):\n",
        "        keep = [f for f in frames if np.random.rand() > drop_prob]\n",
        "        if len(keep) < self.num_frames:\n",
        "            pad = [keep[-1]] * (self.num_frames - len(keep))\n",
        "            keep += pad\n",
        "        return keep[:self.num_frames]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T22:42:50.698091Z",
          "iopub.execute_input": "2025-08-06T22:42:50.698449Z",
          "iopub.status.idle": "2025-08-06T22:42:50.711030Z",
          "shell.execute_reply.started": "2025-08-06T22:42:50.698420Z",
          "shell.execute_reply": "2025-08-06T22:42:50.710287Z"
        },
        "id": "2XVCfcFuDo9N"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building models from scratch"
      ],
      "metadata": {
        "id": "O51tPQVMDo9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 3D CNN Simplified"
      ],
      "metadata": {
        "id": "_fOFCweFDo9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Simple3DCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv3d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(2),\n",
        "\n",
        "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(2),\n",
        "\n",
        "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool3d((1,1,1)),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # x: (B, C, T, H, W)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "        )\n",
        "        self.fc = nn.Linear(128, embed_dim)\n",
        "\n",
        "    def forward(self, x):  # x: (B, T, C, H, W)\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        x = self.conv(x)  # (B*T, 128, 1,1)\n",
        "        x = x.view(B * T, -1)  # (B*T, 128)\n",
        "        x = self.fc(x)  # (B*T, embed_dim)\n",
        "        x = x.view(B, T, -1)\n",
        "        return x  # (B, T, embed_dim)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T22:39:28.253411Z",
          "iopub.execute_input": "2025-08-06T22:39:28.253649Z",
          "iopub.status.idle": "2025-08-06T22:39:28.269627Z",
          "shell.execute_reply.started": "2025-08-06T22:39:28.253631Z",
          "shell.execute_reply": "2025-08-06T22:39:28.268834Z"
        },
        "id": "iM5zEWUnDo9O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. CNN + RNN"
      ],
      "metadata": {
        "id": "mKX0WYXHDo9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_RNN(nn.Module):\n",
        "    def __init__(self, hidden_dim=128, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.encoder = CNNEncoder(embed_dim=256)\n",
        "        self.lstm = nn.LSTM(input_size=256, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):  # x: (B, T, C, H, W)\n",
        "        seq = self.encoder(x)  # (B, T, 256)\n",
        "        out, _ = self.lstm(seq)  # (B, T, hidden_dim)\n",
        "        last = out[:, -1, :]  # (B, hidden_dim)\n",
        "        logits = self.classifier(last)\n",
        "        return logits"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T22:39:48.191300Z",
          "iopub.execute_input": "2025-08-06T22:39:48.191946Z",
          "iopub.status.idle": "2025-08-06T22:39:48.199811Z",
          "shell.execute_reply.started": "2025-08-06T22:39:48.191920Z",
          "shell.execute_reply": "2025-08-06T22:39:48.198812Z"
        },
        "id": "AWMYoyyEDo9P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Transformer"
      ],
      "metadata": {
        "id": "241qdqylDo9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "class VideoTransformer(nn.Module):\n",
        "    def __init__(self, cnn_embed_dim=512, n_heads=4, n_layers=2, hidden_dim=256, num_classes=2, dropout=0.1):\n",
        "        super(VideoTransformer, self).__init__()\n",
        "\n",
        "        # CNN backbone\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Remove FC layer\n",
        "        self.cnn_fc = nn.Linear(resnet.fc.in_features, cnn_embed_dim)\n",
        "\n",
        "        # Positional encoding for frames\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 100, cnn_embed_dim))\n",
        "\n",
        "        # Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=cnn_embed_dim,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=hidden_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        # Classification head\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, cnn_embed_dim))\n",
        "        self.classifier = nn.Linear(cnn_embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):  # x: (B, T, C, H, W)\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        features = self.cnn(x)  # (B*T, 512, 1, 1)\n",
        "        features = features.view(B, T, -1)  # (B, T, 512)\n",
        "        features = self.cnn_fc(features)  # (B, T, cnn_embed_dim)\n",
        "\n",
        "        # Add cls token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, cnn_embed_dim)\n",
        "        tokens = torch.cat([cls_tokens, features], dim=1)  # (B, T+1, cnn_embed_dim)\n",
        "\n",
        "        # Add positional encoding\n",
        "        tokens = tokens + self.pos_embedding[:, :tokens.size(1), :]  # (B, T+1, cnn_embed_dim)\n",
        "\n",
        "        x = self.transformer(tokens)  # (B, T+1, cnn_embed_dim)\n",
        "        cls_out = x[:, 0]  # Take output of cls token\n",
        "        return self.classifier(cls_out)  # (B, num_classes)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-BoZF3NfDo9P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING & EVALUATION"
      ],
      "metadata": {
        "id": "ly5wCLqyDo9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.save_checkpoint(model)\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            if self.restore_best_weights:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        self.best_weights = model.state_dict().copy()\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = len(loader)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch_idx, (videos, labels) in enumerate(loader):\n",
        "        videos = videos.float().to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(videos)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Store predictions & labels for metrics\n",
        "        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Print progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'  Batch [{batch_idx}/{num_batches}] Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / num_batches\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return avg_loss, accuracy, f1\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize metrics\n",
        "    acc_metric = Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
        "    prec_metric = Precision(task=\"multiclass\", average='macro', num_classes=2).to(device)\n",
        "    recall_metric = Recall(task=\"multiclass\", average='macro', num_classes=2).to(device)\n",
        "    f1_metric = F1Score(task=\"multiclass\", average='macro', num_classes=2).to(device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for videos, labels in loader:\n",
        "            videos = videos.float().to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(videos)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Update metrics\n",
        "            acc_metric.update(preds, labels)\n",
        "            prec_metric.update(preds, labels)\n",
        "            recall_metric.update(preds, labels)\n",
        "            f1_metric.update(preds, labels)\n",
        "\n",
        "    return {\n",
        "        \"loss\": total_loss / len(loader),\n",
        "        \"accuracy\": acc_metric.compute().item(),\n",
        "        \"precision\": prec_metric.compute().item(),\n",
        "        \"recall\": recall_metric.compute().item(),\n",
        "        \"f1\": f1_metric.compute().item()\n",
        "    }\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T22:39:52.439585Z",
          "iopub.execute_input": "2025-08-06T22:39:52.439898Z",
          "iopub.status.idle": "2025-08-06T22:39:52.451140Z",
          "shell.execute_reply.started": "2025-08-06T22:39:52.439873Z",
          "shell.execute_reply": "2025-08-06T22:39:52.450445Z"
        },
        "id": "-kqExChFDo9P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREPARATION"
      ],
      "metadata": {
        "id": "oQBbT9lBDo9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(data_root, test_size=0.3, val_size=0.5, random_state=42):\n",
        "    class_names = [\"non shop lifters\", \"shop lifters\"]\n",
        "    label_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "\n",
        "    # Collect video paths and labels\n",
        "    video_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for cls in class_names:\n",
        "        cls_dir = os.path.join(data_root, cls)\n",
        "        if not os.path.isdir(cls_dir):\n",
        "            raise FileNotFoundError(f\"Directory not found: {cls_dir}\")\n",
        "\n",
        "        for video_file in glob.glob(os.path.join(cls_dir, \"*\")):\n",
        "            if video_file.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\")):\n",
        "                video_paths.append(video_file)\n",
        "                labels.append(label_map[cls])\n",
        "\n",
        "    print(f\"Found {len(video_paths)} videos total\")\n",
        "    print(f\"Class distribution: {Counter(labels)}\")\n",
        "\n",
        "    # Stratified split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        video_paths, labels, stratify=labels, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, stratify=temp_labels, test_size=val_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    print(f\"Data split - Train: {len(train_paths)}, Val: {len(val_paths)}, Test: {len(test_paths)}\")\n",
        "\n",
        "    return (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels)\n",
        "\n",
        "def create_balanced_sampler(labels):\n",
        "    class_sample_count = np.bincount(labels)\n",
        "    weights = 1.0 / class_sample_count\n",
        "    sample_weights = [weights[label] for label in labels]\n",
        "    return WeightedRandomSampler(sample_weights, num_samples=len(labels), replacement=True)\n",
        "\n",
        "def create_transforms():\n",
        "    train_transform = Compose([\n",
        "        Resize(112, 112),\n",
        "        HorizontalFlip(p=0.5),\n",
        "        RandomBrightnessContrast(p=0.3),\n",
        "        ToTensorV2(),\n",
        "    ], p=1.0)\n",
        "\n",
        "    val_transform = Compose([\n",
        "        Resize(112, 112),\n",
        "        ToTensorV2(),\n",
        "    ], p=1.0)\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "def create_dataloaders(train_data, val_data, test_data, train_transform, val_transform,\n",
        "                       num_frames=16, batch_size=4, num_workers=2):\n",
        "\n",
        "    train_paths, train_labels = train_data\n",
        "    val_paths, val_labels = val_data\n",
        "    test_paths, test_labels = test_data\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TheftVideoDataset(train_paths, train_labels, num_frames, train_transform, mode=\"train\")\n",
        "    val_dataset = TheftVideoDataset(val_paths, val_labels, num_frames, val_transform, mode=\"val\")\n",
        "    test_dataset = TheftVideoDataset(test_paths, test_labels, num_frames, val_transform, mode=\"test\")\n",
        "\n",
        "    # Create balanced sampler for training set\n",
        "    sampler = create_balanced_sampler(train_labels)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler,\n",
        "                             num_workers=num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                           num_workers=num_workers, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                            num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T22:39:56.776994Z",
          "iopub.execute_input": "2025-08-06T22:39:56.777559Z",
          "iopub.status.idle": "2025-08-06T22:39:56.789074Z",
          "shell.execute_reply.started": "2025-08-06T22:39:56.777532Z",
          "shell.execute_reply": "2025-08-06T22:39:56.788140Z"
        },
        "id": "PhD9MPrkDo9Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, history['train_loss'], label='Train Loss', color='b')\n",
        "    plt.plot(epochs, history['val_loss'], label='Val Loss', color='m')\n",
        "    plt.xlabel(\"Epochs\");\n",
        "    plt.ylabel(\"Loss\");\n",
        "    plt.legend();\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, history['train_accuracy'], label='Train Accuracy', color='b')\n",
        "    plt.plot(epochs, history['val_accuracy'], label='Val Accuracy', color='m')\n",
        "    plt.xlabel(\"Epochs\");\n",
        "    plt.ylabel(\"Accuracy\");\n",
        "    plt.legend();\n",
        "    plt.title(\"Accuracy\")\n",
        "\n",
        "    # F1 Score\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, history['train_f1'], label='Train F1', color='b')\n",
        "    plt.plot(epochs, history['val_f1'], label='Val F1', color='m')\n",
        "    plt.xlabel(\"Epochs\");\n",
        "    plt.ylabel(\"F1 Score\");\n",
        "    plt.legend();\n",
        "    plt.title(\"F1 Score\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(model, dataloader, class_names, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for videos, labels in dataloader:\n",
        "            videos = videos.float().to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(videos)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute confusion matrix & F1\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
        "\n",
        "    # Plot\n",
        "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'Confusion Matrix (F1 Score: {f1:.4f})')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9fj-VOTUq4U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN TRAINING FUNCTION"
      ],
      "metadata": {
        "id": "EYzFspWhDo9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(data_root, model_name=\"Simple3DCNN\", num_epochs=50, learning_rate=1e-4,\n",
        "                batch_size=4, num_frames=16, patience=7):\n",
        "\n",
        "    # Setup device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Prepare data\n",
        "    train_data, val_data, test_data = prepare_data(data_root)\n",
        "    train_transform, val_transform = create_transforms()\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(\n",
        "        train_data, val_data, test_data, train_transform, val_transform,\n",
        "        num_frames, batch_size\n",
        "    )\n",
        "\n",
        "    # Calculate class weights\n",
        "    train_paths, train_labels = train_data\n",
        "    counter = Counter(train_labels)\n",
        "    class_names = [\"non shop lifters\", \"shop lifters\"]\n",
        "    class_counts = [counter[i] for i in range(len(class_names))]\n",
        "    weights = torch.tensor([1.0 / c if c > 0 else 0.0 for c in class_counts])\n",
        "    weights = weights / weights.max()\n",
        "    print(f\"Class weights: {weights}\")\n",
        "\n",
        "    # Initialize model\n",
        "    if model_name == \"Simple3DCNN\":\n",
        "\n",
        "        model = Simple3DCNN(num_classes=2).to(device)\n",
        "    elif model_name == \"CNN_RNN\":\n",
        "        model = CNN_RNN(num_classes=2).to(device)\n",
        "    elif model_name == \"Transformer\":\n",
        "        model = VideoTransformer(num_classes=2).to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "    # Setup training components\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
        "    early_stopping = EarlyStopping(patience=patience, min_delta=0.001)\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'train_accuracy': [], 'train_f1': [],\n",
        "        'val_loss': [], 'val_accuracy': [],\n",
        "        'val_precision': [], 'val_recall': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Training phase\n",
        "        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_metrics[\"loss\"])\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_accuracy'].append(train_acc)\n",
        "        history['train_f1'].append(train_f1)\n",
        "\n",
        "        history['val_loss'].append(val_metrics[\"loss\"])\n",
        "        history['val_accuracy'].append(val_metrics[\"accuracy\"])\n",
        "        history['val_precision'].append(val_metrics[\"precision\"])\n",
        "        history['val_recall'].append(val_metrics[\"recall\"])\n",
        "        history['val_f1'].append(val_metrics[\"f1\"])\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
        "        print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
        "        print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"Val F1: {val_metrics['f1']:.4f}\")\n",
        "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_metrics[\"f1\"] > best_f1:\n",
        "            best_f1 = val_metrics[\"f1\"]\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_f1': best_f1,\n",
        "                'history': history\n",
        "            }, \"best_model.pth\")\n",
        "            print(f\">>> New best model saved! F1: {best_f1:.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if early_stopping(val_metrics[\"loss\"], model):\n",
        "            print(f\"Early stopping triggered after {epoch} epochs\")\n",
        "            break\n",
        "\n",
        "    # Load best model for testing\n",
        "    checkpoint = torch.load(\"best_model.pth\")\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL EVALUATION ON TEST SET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test_metrics = evaluate(model, test_loader, criterion, device)\n",
        "    print(\"Test Results:\")\n",
        "    for key, value in test_metrics.items():\n",
        "        print(f\"  {key.capitalize()}: {value:.4f}\")\n",
        "\n",
        "    # Visualize evolution\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Plot Confusion matrix\n",
        "    plot_confusion_matrix(model, test_loader, class_names, device)\n",
        "\n",
        "    return model, history, test_metrics"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T22:40:02.395390Z",
          "iopub.execute_input": "2025-08-06T22:40:02.395730Z",
          "iopub.status.idle": "2025-08-06T22:40:02.410388Z",
          "shell.execute_reply.started": "2025-08-06T22:40:02.395708Z",
          "shell.execute_reply": "2025-08-06T22:40:02.409371Z"
        },
        "id": "Ie2Ie4tdDo9Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREDICTION FUNCTION"
      ],
      "metadata": {
        "id": "mV8uS_uMDo9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_video(model, video_path, transform, num_frames=16, device='cpu', class_names=None):\n",
        "    \"\"\"Predict the class of a single video\"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [\"non shop lifters\", \"shop lifters\"]\n",
        "\n",
        "    # Create temporary dataset with dummy label\n",
        "    dataset = TheftVideoDataset([video_path], [0], num_frames=num_frames, transform=transform)\n",
        "    video_tensor, _ = dataset[0]\n",
        "    video_tensor = video_tensor.unsqueeze(0).float().to(device)  # Add batch dimension\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(video_tensor)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        pred = torch.argmax(probs, dim=1).item()\n",
        "        confidence = probs.max().item()\n",
        "\n",
        "    return {\n",
        "        'prediction': class_names[pred],\n",
        "        'confidence': confidence,\n",
        "        'probabilities': probs.squeeze().cpu().tolist()\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T22:40:06.417163Z",
          "iopub.execute_input": "2025-08-06T22:40:06.418319Z",
          "iopub.status.idle": "2025-08-06T22:40:06.424520Z",
          "shell.execute_reply.started": "2025-08-06T22:40:06.418282Z",
          "shell.execute_reply": "2025-08-06T22:40:06.423633Z"
        },
        "id": "xogSI0zmDo9R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    DATA_ROOT = \"/kaggle/input/shop-dataset/Shop DataSet\"\n",
        "    MODEL_NAME = \"Simple3DCNN\"\n",
        "\n",
        "    # Training parameters\n",
        "    config = {\n",
        "        'num_epochs': 50,\n",
        "        'learning_rate': 1e-4,\n",
        "        'batch_size': 4,\n",
        "        'num_frames': 16,\n",
        "        'patience': 7\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Train the model\n",
        "        model, history, test_metrics = train_model(DATA_ROOT, MODEL_NAME, **config)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T16:29:03.196490Z",
          "iopub.execute_input": "2025-08-06T16:29:03.196745Z",
          "iopub.status.idle": "2025-08-06T19:05:30.563599Z",
          "shell.execute_reply.started": "2025-08-06T16:29:03.196727Z",
          "shell.execute_reply": "2025-08-06T19:05:30.562713Z"
        },
        "id": "H6t6BW-iDo9R",
        "outputId": "b9ee5435-8a6d-484d-d95a-67de5c7995d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\nFound 855 videos total\nClass distribution: Counter({0: 531, 1: 324})\nData split - Train: 598, Val: 128, Test: 129\nClass weights: tensor([0.6119, 1.0000])\n\nStarting training for 50 epochs...\n============================================================\n\nEpoch 1/50\n------------------------------\n  Batch [0/150] Loss: 0.6942\n  Batch [10/150] Loss: 0.6727\n  Batch [20/150] Loss: 0.6252\n  Batch [30/150] Loss: 0.7107\n  Batch [40/150] Loss: 0.4635\n  Batch [50/150] Loss: 0.6217\n  Batch [60/150] Loss: 0.7100\n  Batch [70/150] Loss: 0.5529\n  Batch [80/150] Loss: 0.6046\n  Batch [90/150] Loss: 0.6267\n  Batch [100/150] Loss: 0.5591\n  Batch [110/150] Loss: 0.7325\n  Batch [120/150] Loss: 0.8061\n  Batch [130/150] Loss: 0.6729\n  Batch [140/150] Loss: 0.5875\nTrain Loss: 0.6779\nVal Loss: 0.7410\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000100\n>>> New best model saved! F1: 0.2727\n\nEpoch 2/50\n------------------------------\n  Batch [0/150] Loss: 0.5279\n  Batch [10/150] Loss: 0.6371\n  Batch [20/150] Loss: 0.7455\n  Batch [30/150] Loss: 0.6362\n  Batch [40/150] Loss: 0.7520\n  Batch [50/150] Loss: 0.7264\n  Batch [60/150] Loss: 0.6362\n  Batch [70/150] Loss: 0.6084\n  Batch [80/150] Loss: 0.8318\n  Batch [90/150] Loss: 0.5720\n  Batch [100/150] Loss: 0.5234\n  Batch [110/150] Loss: 0.7618\n  Batch [120/150] Loss: 0.5478\n  Batch [130/150] Loss: 0.5355\n  Batch [140/150] Loss: 0.5716\nTrain Loss: 0.6661\nVal Loss: 0.7985\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000100\n\nEpoch 3/50\n------------------------------\n  Batch [0/150] Loss: 0.4973\n  Batch [10/150] Loss: 0.5050\n  Batch [20/150] Loss: 0.8445\n  Batch [30/150] Loss: 0.6830\n  Batch [40/150] Loss: 0.5173\n  Batch [50/150] Loss: 0.6408\n  Batch [60/150] Loss: 0.7900\n  Batch [70/150] Loss: 0.7766\n  Batch [80/150] Loss: 0.6106\n  Batch [90/150] Loss: 0.7686\n  Batch [100/150] Loss: 0.6506\n  Batch [110/150] Loss: 0.6652\n  Batch [120/150] Loss: 0.6214\n  Batch [130/150] Loss: 0.6491\n  Batch [140/150] Loss: 0.8131\nTrain Loss: 0.6606\nVal Loss: 0.6801\nVal Accuracy: 0.4453\nVal F1: 0.4058\nLearning Rate: 0.000100\n>>> New best model saved! F1: 0.4058\n\nEpoch 4/50\n------------------------------\n  Batch [0/150] Loss: 0.5922\n  Batch [10/150] Loss: 0.5936\n  Batch [20/150] Loss: 0.6258\n  Batch [30/150] Loss: 0.5007\n  Batch [40/150] Loss: 0.5967\n  Batch [50/150] Loss: 0.4943\n  Batch [60/150] Loss: 0.5801\n  Batch [70/150] Loss: 0.7464\n  Batch [80/150] Loss: 0.5283\n  Batch [90/150] Loss: 0.5503\n  Batch [100/150] Loss: 0.6879\n  Batch [110/150] Loss: 0.7412\n  Batch [120/150] Loss: 0.7496\n  Batch [130/150] Loss: 0.5465\n  Batch [140/150] Loss: 0.5929\nTrain Loss: 0.6509\nVal Loss: 0.8049\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000100\n\nEpoch 5/50\n------------------------------\n  Batch [0/150] Loss: 0.7962\n  Batch [10/150] Loss: 0.6691\n  Batch [20/150] Loss: 0.6780\n  Batch [30/150] Loss: 0.6503\n  Batch [40/150] Loss: 0.7605\n  Batch [50/150] Loss: 0.6758\n  Batch [60/150] Loss: 0.5204\n  Batch [70/150] Loss: 0.7542\n  Batch [80/150] Loss: 0.7898\n  Batch [90/150] Loss: 0.5880\n  Batch [100/150] Loss: 0.6286\n  Batch [110/150] Loss: 0.7406\n  Batch [120/150] Loss: 0.5571\n  Batch [130/150] Loss: 0.5016\n  Batch [140/150] Loss: 0.9266\nTrain Loss: 0.6569\nVal Loss: 0.7269\nVal Accuracy: 0.6250\nVal F1: 0.3846\nLearning Rate: 0.000100\n\nEpoch 6/50\n------------------------------\n  Batch [0/150] Loss: 0.5891\n  Batch [10/150] Loss: 0.6492\n  Batch [20/150] Loss: 0.6833\n  Batch [30/150] Loss: 0.7893\n  Batch [40/150] Loss: 0.5394\n  Batch [50/150] Loss: 0.6929\n  Batch [60/150] Loss: 0.7425\n  Batch [70/150] Loss: 0.6142\n  Batch [80/150] Loss: 0.8602\n  Batch [90/150] Loss: 0.5596\n  Batch [100/150] Loss: 0.6472\n  Batch [110/150] Loss: 0.7713\n  Batch [120/150] Loss: 0.7153\n  Batch [130/150] Loss: 0.6208\n  Batch [140/150] Loss: 0.5808\nTrain Loss: 0.6654\nVal Loss: 0.6693\nVal Accuracy: 0.4219\nVal F1: 0.3660\nLearning Rate: 0.000100\n\nEpoch 7/50\n------------------------------\n  Batch [0/150] Loss: 0.6193\n  Batch [10/150] Loss: 0.7100\n  Batch [20/150] Loss: 0.8797\n  Batch [30/150] Loss: 0.6021\n  Batch [40/150] Loss: 0.5505\n  Batch [50/150] Loss: 0.5109\n  Batch [60/150] Loss: 0.6366\n  Batch [70/150] Loss: 0.6800\n  Batch [80/150] Loss: 0.4596\n  Batch [90/150] Loss: 0.5167\n  Batch [100/150] Loss: 0.5672\n  Batch [110/150] Loss: 0.6970\n  Batch [120/150] Loss: 0.5053\n  Batch [130/150] Loss: 0.8064\n  Batch [140/150] Loss: 0.7326\nTrain Loss: 0.6455\nVal Loss: 0.7428\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000100\n\nEpoch 8/50\n------------------------------\n  Batch [0/150] Loss: 0.7209\n  Batch [10/150] Loss: 0.6567\n  Batch [20/150] Loss: 0.6542\n  Batch [30/150] Loss: 0.5815\n  Batch [40/150] Loss: 0.5284\n  Batch [50/150] Loss: 0.6542\n  Batch [60/150] Loss: 0.6780\n  Batch [70/150] Loss: 0.5554\n  Batch [80/150] Loss: 0.5490\n  Batch [90/150] Loss: 0.7684\n  Batch [100/150] Loss: 0.4601\n  Batch [110/150] Loss: 0.8153\n  Batch [120/150] Loss: 0.4691\n  Batch [130/150] Loss: 0.7764\n  Batch [140/150] Loss: 0.6283\nTrain Loss: 0.6344\nVal Loss: 0.6439\nVal Accuracy: 0.6562\nVal F1: 0.4971\nLearning Rate: 0.000100\n>>> New best model saved! F1: 0.4971\n\nEpoch 9/50\n------------------------------\n  Batch [0/150] Loss: 0.7907\n  Batch [10/150] Loss: 0.5700\n  Batch [20/150] Loss: 0.9299\n  Batch [30/150] Loss: 0.5388\n  Batch [40/150] Loss: 0.5716\n  Batch [50/150] Loss: 0.7001\n  Batch [60/150] Loss: 0.5532\n  Batch [70/150] Loss: 0.5927\n  Batch [80/150] Loss: 0.5672\n  Batch [90/150] Loss: 0.7297\n  Batch [100/150] Loss: 0.6103\n  Batch [110/150] Loss: 0.6442\n  Batch [120/150] Loss: 0.7366\n  Batch [130/150] Loss: 0.7469\n  Batch [140/150] Loss: 0.5470\nTrain Loss: 0.6333\nVal Loss: 0.6309\nVal Accuracy: 0.8203\nVal F1: 0.8132\nLearning Rate: 0.000100\n>>> New best model saved! F1: 0.8132\n\nEpoch 10/50\n------------------------------\n  Batch [0/150] Loss: 0.4579\n  Batch [10/150] Loss: 0.3826\n  Batch [20/150] Loss: 0.6731\n  Batch [30/150] Loss: 0.5014\n  Batch [40/150] Loss: 0.8253\n  Batch [50/150] Loss: 0.6251\n  Batch [60/150] Loss: 0.6413\n  Batch [70/150] Loss: 0.5678\n  Batch [80/150] Loss: 0.4414\n  Batch [90/150] Loss: 0.5030\n  Batch [100/150] Loss: 0.5685\n  Batch [110/150] Loss: 0.7395\n  Batch [120/150] Loss: 0.5202\n  Batch [130/150] Loss: 0.5751\n  Batch [140/150] Loss: 0.6324\nTrain Loss: 0.5978\nVal Loss: 1.3289\nVal Accuracy: 0.6250\nVal F1: 0.3846\nLearning Rate: 0.000100\n\nEpoch 11/50\n------------------------------\n  Batch [0/150] Loss: 0.4660\n  Batch [10/150] Loss: 0.4079\n  Batch [20/150] Loss: 0.4189\n  Batch [30/150] Loss: 0.7747\n  Batch [40/150] Loss: 0.5610\n  Batch [50/150] Loss: 0.5056\n  Batch [60/150] Loss: 0.4376\n  Batch [70/150] Loss: 0.3324\n  Batch [80/150] Loss: 0.9544\n  Batch [90/150] Loss: 0.5063\n  Batch [100/150] Loss: 0.3626\n  Batch [110/150] Loss: 0.8072\n  Batch [120/150] Loss: 0.6303\n  Batch [130/150] Loss: 0.9146\n  Batch [140/150] Loss: 0.4366\nTrain Loss: 0.5832\nVal Loss: 1.2964\nVal Accuracy: 0.6250\nVal F1: 0.3846\nLearning Rate: 0.000100\n\nEpoch 12/50\n------------------------------\n  Batch [0/150] Loss: 0.4808\n  Batch [10/150] Loss: 0.2315\n  Batch [20/150] Loss: 0.2935\n  Batch [30/150] Loss: 0.6634\n  Batch [40/150] Loss: 0.3146\n  Batch [50/150] Loss: 0.3405\n  Batch [60/150] Loss: 0.4342\n  Batch [70/150] Loss: 0.6161\n  Batch [80/150] Loss: 0.6053\n  Batch [90/150] Loss: 0.6884\n  Batch [100/150] Loss: 0.4165\n  Batch [110/150] Loss: 0.6036\n  Batch [120/150] Loss: 0.4800\n  Batch [130/150] Loss: 0.6161\n  Batch [140/150] Loss: 0.3618\nTrain Loss: 0.5098\nVal Loss: 4.2266\nVal Accuracy: 0.6250\nVal F1: 0.3846\nLearning Rate: 0.000100\n\nEpoch 13/50\n------------------------------\n  Batch [0/150] Loss: 0.5131\n  Batch [10/150] Loss: 0.4440\n  Batch [20/150] Loss: 0.5980\n  Batch [30/150] Loss: 0.3793\n  Batch [40/150] Loss: 0.5178\n  Batch [50/150] Loss: 0.4333\n  Batch [60/150] Loss: 0.6147\n  Batch [70/150] Loss: 0.3756\n  Batch [80/150] Loss: 0.3024\n  Batch [90/150] Loss: 0.5096\n  Batch [100/150] Loss: 0.1956\n  Batch [110/150] Loss: 0.2289\n  Batch [120/150] Loss: 0.2538\n  Batch [130/150] Loss: 0.2207\n  Batch [140/150] Loss: 0.2962\nTrain Loss: 0.4483\nVal Loss: 4.6717\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000050\n\nEpoch 14/50\n------------------------------\n  Batch [0/150] Loss: 0.1433\n  Batch [10/150] Loss: 0.4643\n  Batch [20/150] Loss: 0.1899\n  Batch [30/150] Loss: 0.4035\n  Batch [40/150] Loss: 0.5380\n  Batch [50/150] Loss: 0.1961\n  Batch [60/150] Loss: 1.0399\n  Batch [70/150] Loss: 0.8385\n  Batch [80/150] Loss: 0.1240\n  Batch [90/150] Loss: 0.6137\n  Batch [100/150] Loss: 0.5364\n  Batch [110/150] Loss: 0.3541\n  Batch [120/150] Loss: 0.2607\n  Batch [130/150] Loss: 0.2021\n  Batch [140/150] Loss: 0.2980\nTrain Loss: 0.3230\nVal Loss: 0.4998\nVal Accuracy: 0.6875\nVal F1: 0.5429\nLearning Rate: 0.000050\n\nEpoch 15/50\n------------------------------\n  Batch [0/150] Loss: 0.2132\n  Batch [10/150] Loss: 0.2820\n  Batch [20/150] Loss: 0.3062\n  Batch [30/150] Loss: 0.7997\n  Batch [40/150] Loss: 0.3369\n  Batch [50/150] Loss: 0.0855\n  Batch [60/150] Loss: 0.2136\n  Batch [70/150] Loss: 0.2578\n  Batch [80/150] Loss: 0.2054\n  Batch [90/150] Loss: 0.9108\n  Batch [100/150] Loss: 0.2814\n  Batch [110/150] Loss: 0.1336\n  Batch [120/150] Loss: 0.6465\n  Batch [130/150] Loss: 0.1565\n  Batch [140/150] Loss: 0.2808\nTrain Loss: 0.3528\nVal Loss: 2.3915\nVal Accuracy: 0.6250\nVal F1: 0.3846\nLearning Rate: 0.000050\n\nEpoch 16/50\n------------------------------\n  Batch [0/150] Loss: 0.3103\n  Batch [10/150] Loss: 0.0828\n  Batch [20/150] Loss: 0.9837\n  Batch [30/150] Loss: 0.2942\n  Batch [40/150] Loss: 0.3938\n  Batch [50/150] Loss: 0.4428\n  Batch [60/150] Loss: 0.2509\n  Batch [70/150] Loss: 0.3598\n  Batch [80/150] Loss: 0.7755\n  Batch [90/150] Loss: 0.1065\n  Batch [100/150] Loss: 0.9362\n  Batch [110/150] Loss: 0.3598\n  Batch [120/150] Loss: 0.6996\n  Batch [130/150] Loss: 0.0885\n  Batch [140/150] Loss: 0.6560\nTrain Loss: 0.3844\nVal Loss: 0.3341\nVal Accuracy: 0.9219\nVal F1: 0.9124\nLearning Rate: 0.000050\n>>> New best model saved! F1: 0.9124\n\nEpoch 17/50\n------------------------------\n  Batch [0/150] Loss: 0.3502\n  Batch [10/150] Loss: 0.2033\n  Batch [20/150] Loss: 0.0730\n  Batch [30/150] Loss: 0.1339\n  Batch [40/150] Loss: 0.2910\n  Batch [50/150] Loss: 0.0938\n  Batch [60/150] Loss: 0.1198\n  Batch [70/150] Loss: 0.1335\n  Batch [80/150] Loss: 0.0861\n  Batch [90/150] Loss: 0.5080\n  Batch [100/150] Loss: 0.5401\n  Batch [110/150] Loss: 0.4757\n  Batch [120/150] Loss: 0.1035\n  Batch [130/150] Loss: 0.0856\n  Batch [140/150] Loss: 0.3807\nTrain Loss: 0.2996\nVal Loss: 1.8414\nVal Accuracy: 0.6250\nVal F1: 0.3846\nLearning Rate: 0.000050\n\nEpoch 18/50\n------------------------------\n  Batch [0/150] Loss: 0.3611\n  Batch [10/150] Loss: 0.1664\n  Batch [20/150] Loss: 0.3750\n  Batch [30/150] Loss: 0.1646\n  Batch [40/150] Loss: 0.2392\n  Batch [50/150] Loss: 0.1040\n  Batch [60/150] Loss: 0.3107\n  Batch [70/150] Loss: 0.2275\n  Batch [80/150] Loss: 0.7202\n  Batch [90/150] Loss: 1.1356\n  Batch [100/150] Loss: 0.0798\n  Batch [110/150] Loss: 0.6448\n  Batch [120/150] Loss: 0.2203\n  Batch [130/150] Loss: 0.1148\n  Batch [140/150] Loss: 0.2001\nTrain Loss: 0.2710\nVal Loss: 0.9278\nVal Accuracy: 0.6328\nVal F1: 0.4069\nLearning Rate: 0.000050\n\nEpoch 19/50\n------------------------------\n  Batch [0/150] Loss: 0.1185\n  Batch [10/150] Loss: 0.3446\n  Batch [20/150] Loss: 0.1784\n  Batch [30/150] Loss: 0.1185\n  Batch [40/150] Loss: 0.8074\n  Batch [50/150] Loss: 0.1303\n  Batch [60/150] Loss: 0.2124\n  Batch [70/150] Loss: 0.2265\n  Batch [80/150] Loss: 0.2206\n  Batch [90/150] Loss: 0.1175\n  Batch [100/150] Loss: 0.0836\n  Batch [110/150] Loss: 0.3869\n  Batch [120/150] Loss: 0.1155\n  Batch [130/150] Loss: 0.4361\n  Batch [140/150] Loss: 0.3367\nTrain Loss: 0.2698\nVal Loss: 0.1872\nVal Accuracy: 0.9922\nVal F1: 0.9917\nLearning Rate: 0.000050\n>>> New best model saved! F1: 0.9917\n\nEpoch 20/50\n------------------------------\n  Batch [0/150] Loss: 0.6143\n  Batch [10/150] Loss: 0.1530\n  Batch [20/150] Loss: 0.1832\n  Batch [30/150] Loss: 0.1638\n  Batch [40/150] Loss: 0.0695\n  Batch [50/150] Loss: 0.2705\n  Batch [60/150] Loss: 0.2135\n  Batch [70/150] Loss: 0.0643\n  Batch [80/150] Loss: 0.0759\n  Batch [90/150] Loss: 0.1036\n  Batch [100/150] Loss: 0.0953\n  Batch [110/150] Loss: 0.0764\n  Batch [120/150] Loss: 0.0353\n  Batch [130/150] Loss: 0.0707\n  Batch [140/150] Loss: 0.3246\nTrain Loss: 0.2588\nVal Loss: 0.2553\nVal Accuracy: 0.8750\nVal F1: 0.8730\nLearning Rate: 0.000050\n\nEpoch 21/50\n------------------------------\n  Batch [0/150] Loss: 0.0701\n  Batch [10/150] Loss: 0.2180\n  Batch [20/150] Loss: 0.3956\n  Batch [30/150] Loss: 0.0449\n  Batch [40/150] Loss: 0.2826\n  Batch [50/150] Loss: 0.1105\n  Batch [60/150] Loss: 0.1356\n  Batch [70/150] Loss: 0.2720\n  Batch [80/150] Loss: 0.0819\n  Batch [90/150] Loss: 0.5212\n  Batch [100/150] Loss: 0.2159\n  Batch [110/150] Loss: 0.2269\n  Batch [120/150] Loss: 0.0618\n  Batch [130/150] Loss: 0.0332\n  Batch [140/150] Loss: 0.0259\nTrain Loss: 0.2400\nVal Loss: 2.0314\nVal Accuracy: 0.6250\nVal F1: 0.3846\nLearning Rate: 0.000050\n\nEpoch 22/50\n------------------------------\n  Batch [0/150] Loss: 0.0673\n  Batch [10/150] Loss: 0.1107\n  Batch [20/150] Loss: 0.0707\n  Batch [30/150] Loss: 0.0813\n  Batch [40/150] Loss: 0.2835\n  Batch [50/150] Loss: 0.1125\n  Batch [60/150] Loss: 0.0235\n  Batch [70/150] Loss: 0.0279\n  Batch [80/150] Loss: 0.0953\n  Batch [90/150] Loss: 0.1901\n  Batch [100/150] Loss: 0.2297\n  Batch [110/150] Loss: 0.5618\n  Batch [120/150] Loss: 0.1831\n  Batch [130/150] Loss: 0.3762\n  Batch [140/150] Loss: 0.2438\nTrain Loss: 0.2298\nVal Loss: 0.6487\nVal Accuracy: 0.5938\nVal F1: 0.5836\nLearning Rate: 0.000050\n\nEpoch 23/50\n------------------------------\n  Batch [0/150] Loss: 0.3729\n  Batch [10/150] Loss: 0.4670\n  Batch [20/150] Loss: 0.9621\n  Batch [30/150] Loss: 0.3005\n  Batch [40/150] Loss: 0.3212\n  Batch [50/150] Loss: 0.7015\n  Batch [60/150] Loss: 0.1531\n  Batch [70/150] Loss: 0.3903\n  Batch [80/150] Loss: 0.0436\n  Batch [90/150] Loss: 0.7525\n  Batch [100/150] Loss: 0.4235\n  Batch [110/150] Loss: 0.2333\n  Batch [120/150] Loss: 0.1698\n  Batch [130/150] Loss: 0.0367\n  Batch [140/150] Loss: 0.1283\nTrain Loss: 0.2326\nVal Loss: 0.5371\nVal Accuracy: 0.6875\nVal F1: 0.5429\nLearning Rate: 0.000025\n\nEpoch 24/50\n------------------------------\n  Batch [0/150] Loss: 1.0951\n  Batch [10/150] Loss: 0.2465\n  Batch [20/150] Loss: 0.1087\n  Batch [30/150] Loss: 0.1622\n  Batch [40/150] Loss: 0.0773\n  Batch [50/150] Loss: 0.1447\n  Batch [60/150] Loss: 0.2672\n  Batch [70/150] Loss: 0.0990\n  Batch [80/150] Loss: 0.3391\n  Batch [90/150] Loss: 0.0948\n  Batch [100/150] Loss: 0.1245\n  Batch [110/150] Loss: 0.0638\n  Batch [120/150] Loss: 0.2627\n  Batch [130/150] Loss: 0.6434\n  Batch [140/150] Loss: 0.2001\nTrain Loss: 0.2387\nVal Loss: 0.7576\nVal Accuracy: 0.6484\nVal F1: 0.4491\nLearning Rate: 0.000025\n\nEpoch 25/50\n------------------------------\n  Batch [0/150] Loss: 0.1540\n  Batch [10/150] Loss: 0.1037\n  Batch [20/150] Loss: 0.3099\n  Batch [30/150] Loss: 0.0449\n  Batch [40/150] Loss: 0.8518\n  Batch [50/150] Loss: 0.0675\n  Batch [60/150] Loss: 0.1466\n  Batch [70/150] Loss: 0.0567\n  Batch [80/150] Loss: 0.3477\n  Batch [90/150] Loss: 0.0484\n  Batch [100/150] Loss: 0.2391\n  Batch [110/150] Loss: 0.1397\n  Batch [120/150] Loss: 0.1410\n  Batch [130/150] Loss: 0.0378\n  Batch [140/150] Loss: 0.0335\nTrain Loss: 0.1839\nVal Loss: 0.4226\nVal Accuracy: 0.7344\nVal F1: 0.7343\nLearning Rate: 0.000025\n\nEpoch 26/50\n------------------------------\n  Batch [0/150] Loss: 0.3433\n  Batch [10/150] Loss: 0.2759\n  Batch [20/150] Loss: 0.0494\n  Batch [30/150] Loss: 0.0762\n  Batch [40/150] Loss: 0.1802\n  Batch [50/150] Loss: 0.7808\n  Batch [60/150] Loss: 0.6668\n  Batch [70/150] Loss: 0.1392\n  Batch [80/150] Loss: 0.2122\n  Batch [90/150] Loss: 0.0677\n  Batch [100/150] Loss: 0.1046\n  Batch [110/150] Loss: 0.1487\n  Batch [120/150] Loss: 0.5093\n  Batch [130/150] Loss: 0.1228\n  Batch [140/150] Loss: 0.9377\nTrain Loss: 0.1968\nVal Loss: 0.6291\nVal Accuracy: 0.6172\nVal F1: 0.6103\nLearning Rate: 0.000025\nEarly stopping triggered after 26 epochs\n\n============================================================\nFINAL EVALUATION ON TEST SET\n============================================================\nTest Results:\n  Loss: 0.1759\n  Accuracy: 0.9922\n  Precision: 0.9900\n  Recall: 0.9937\n  F1: 0.9918\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3KwVtxxQPAY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "# Configuration\n",
        "DATA_ROOT = \"/kaggle/input/shop-dataset/Shop DataSet\"\n",
        "MODEL_NAME = \"CNN_RNN\"\n",
        "\n",
        "# Training parameters\n",
        "config = {\n",
        "    'num_epochs': 50,\n",
        "    'learning_rate': 1e-4,\n",
        "    'batch_size': 4,\n",
        "    'num_frames': 16,\n",
        "    'patience': 7\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    model, history, test_metrics = train_model(DATA_ROOT, MODEL_NAME, **config)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T20:43:36.581145Z",
          "iopub.execute_input": "2025-08-06T20:43:36.581727Z",
          "iopub.status.idle": "2025-08-06T21:40:15.193640Z",
          "shell.execute_reply.started": "2025-08-06T20:43:36.581702Z",
          "shell.execute_reply": "2025-08-06T21:40:15.192502Z"
        },
        "id": "sBCFcJoZDo9S",
        "outputId": "c53c2ed3-ec71-4df7-f014-4dd8aca5869e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\nFound 855 videos total\nClass distribution: Counter({0: 531, 1: 324})\nData split - Train: 598, Val: 128, Test: 129\nClass weights: tensor([0.6119, 1.0000])\n\nStarting training for 50 epochs...\n============================================================\n\nEpoch 1/50\n------------------------------\n  Batch [0/150] Loss: 0.6515\n  Batch [10/150] Loss: 0.8523\n  Batch [20/150] Loss: 0.5962\n  Batch [30/150] Loss: 0.8148\n  Batch [40/150] Loss: 0.6636\n  Batch [50/150] Loss: 0.4651\n  Batch [60/150] Loss: 0.5565\n  Batch [70/150] Loss: 0.7496\n  Batch [80/150] Loss: 0.7187\n  Batch [90/150] Loss: 0.7132\n  Batch [100/150] Loss: 0.6817\n  Batch [110/150] Loss: 0.6486\n  Batch [120/150] Loss: 0.6745\n  Batch [130/150] Loss: 0.6032\n  Batch [140/150] Loss: 0.6745\nTrain Loss: 0.6912\nVal Loss: 0.7046\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000100\n>>> New best model saved! F1: 0.2727\n\nEpoch 2/50\n------------------------------\n  Batch [0/150] Loss: 0.6743\n  Batch [10/150] Loss: 0.7939\n  Batch [20/150] Loss: 0.7276\n  Batch [30/150] Loss: 0.6301\n  Batch [40/150] Loss: 0.7330\n  Batch [50/150] Loss: 0.7330\n  Batch [60/150] Loss: 0.6727\n  Batch [70/150] Loss: 0.5978\n  Batch [80/150] Loss: 0.7238\n  Batch [90/150] Loss: 0.6402\n  Batch [100/150] Loss: 0.6369\n  Batch [110/150] Loss: 0.8026\n  Batch [120/150] Loss: 0.6727\n  Batch [130/150] Loss: 0.7335\n  Batch [140/150] Loss: 0.6238\nTrain Loss: 0.6860\nVal Loss: 0.7092\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000100\n\nEpoch 3/50\n------------------------------\n  Batch [0/150] Loss: 0.8244\n  Batch [10/150] Loss: 0.6222\n  Batch [20/150] Loss: 0.6709\n  Batch [30/150] Loss: 0.6695\n  Batch [40/150] Loss: 0.6690\n  Batch [50/150] Loss: 0.6687\n  Batch [60/150] Loss: 0.6693\n  Batch [70/150] Loss: 0.6136\n  Batch [80/150] Loss: 0.5705\n  Batch [90/150] Loss: 0.6699\n  Batch [100/150] Loss: 0.6164\n  Batch [110/150] Loss: 0.6710\n  Batch [120/150] Loss: 0.8224\n  Batch [130/150] Loss: 0.5839\n  Batch [140/150] Loss: 0.6224\nTrain Loss: 0.6850\nVal Loss: 0.7084\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000100\n\nEpoch 4/50\n------------------------------\n  Batch [0/150] Loss: 0.6211\n  Batch [10/150] Loss: 0.7396\n  Batch [20/150] Loss: 0.6689\n  Batch [30/150] Loss: 0.5545\n  Batch [40/150] Loss: 0.6060\n  Batch [50/150] Loss: 0.6681\n  Batch [60/150] Loss: 0.6679\n  Batch [70/150] Loss: 0.6674\n  Batch [80/150] Loss: 0.5433\n  Batch [90/150] Loss: 0.8833\n  Batch [100/150] Loss: 0.8766\n  Batch [110/150] Loss: 0.6670\n  Batch [120/150] Loss: 0.7553\n  Batch [130/150] Loss: 0.5969\n  Batch [140/150] Loss: 0.6667\nTrain Loss: 0.6727\nVal Loss: 0.7192\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000100\n\nEpoch 5/50\n------------------------------\n  Batch [0/150] Loss: 0.5371\n  Batch [10/150] Loss: 0.6661\n  Batch [20/150] Loss: 0.6659\n  Batch [30/150] Loss: 0.5874\n  Batch [40/150] Loss: 0.6653\n  Batch [50/150] Loss: 0.7664\n  Batch [60/150] Loss: 0.6655\n  Batch [70/150] Loss: 0.5905\n  Batch [80/150] Loss: 0.6658\n  Batch [90/150] Loss: 0.8888\n  Batch [100/150] Loss: 0.5915\n  Batch [110/150] Loss: 0.6660\n  Batch [120/150] Loss: 0.7550\n  Batch [130/150] Loss: 0.7445\n  Batch [140/150] Loss: 0.7424\nTrain Loss: 0.6810\nVal Loss: 0.7115\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000050\n\nEpoch 6/50\n------------------------------\n  Batch [0/150] Loss: 0.7425\n  Batch [10/150] Loss: 0.6109\n  Batch [20/150] Loss: 0.7450\n  Batch [30/150] Loss: 0.6688\n  Batch [40/150] Loss: 0.7463\n  Batch [50/150] Loss: 0.6057\n  Batch [60/150] Loss: 0.5475\n  Batch [70/150] Loss: 0.6666\n  Batch [80/150] Loss: 0.5477\n  Batch [90/150] Loss: 0.6675\n  Batch [100/150] Loss: 0.7516\n  Batch [110/150] Loss: 0.6023\n  Batch [120/150] Loss: 0.6677\n  Batch [130/150] Loss: 0.5505\n  Batch [140/150] Loss: 0.6021\nTrain Loss: 0.6772\nVal Loss: 0.7167\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000050\n\nEpoch 7/50\n------------------------------\n  Batch [0/150] Loss: 0.6674\n  Batch [10/150] Loss: 0.7535\n  Batch [20/150] Loss: 0.6673\n  Batch [30/150] Loss: 0.6672\n  Batch [40/150] Loss: 0.5969\n  Batch [50/150] Loss: 0.5964\n  Batch [60/150] Loss: 0.5985\n  Batch [70/150] Loss: 0.7520\n  Batch [80/150] Loss: 0.6671\n  Batch [90/150] Loss: 0.7532\n  Batch [100/150] Loss: 0.6676\n  Batch [110/150] Loss: 0.6676\n  Batch [120/150] Loss: 0.6673\n  Batch [130/150] Loss: 0.5976\n  Batch [140/150] Loss: 0.6670\nTrain Loss: 0.6781\nVal Loss: 0.7182\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000050\n\nEpoch 8/50\n------------------------------\n  Batch [0/150] Loss: 0.6669\n  Batch [10/150] Loss: 0.5377\n  Batch [20/150] Loss: 0.5923\n  Batch [30/150] Loss: 0.5277\n  Batch [40/150] Loss: 0.5894\n  Batch [50/150] Loss: 0.5895\n  Batch [60/150] Loss: 0.7631\n  Batch [70/150] Loss: 0.5902\n  Batch [80/150] Loss: 0.7654\n  Batch [90/150] Loss: 0.6653\n  Batch [100/150] Loss: 0.5848\n  Batch [110/150] Loss: 0.5833\n  Batch [120/150] Loss: 0.5845\n  Batch [130/150] Loss: 0.6655\n  Batch [140/150] Loss: 0.6653\nTrain Loss: 0.6684\nVal Loss: 0.7237\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000050\nEarly stopping triggered after 8 epochs\n\n============================================================\nFINAL EVALUATION ON TEST SET\n============================================================\nTest Results:\n  Loss: 0.7052\n  Accuracy: 0.3798\n  Precision: 0.1899\n  Recall: 0.5000\n  F1: 0.2753\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "# Configuration\n",
        "DATA_ROOT = \"/kaggle/input/shop-dataset/Shop DataSet\"\n",
        "MODEL_NAME = \"Transformer\"\n",
        "\n",
        "# Training parameters\n",
        "config = {\n",
        "    'num_epochs': 50,\n",
        "    'learning_rate': 1e-4,\n",
        "    'batch_size': 4,\n",
        "    'num_frames': 16,\n",
        "    'patience': 7\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    model, history, test_metrics = train_model(DATA_ROOT, MODEL_NAME, **config)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-06T22:43:04.185792Z",
          "iopub.execute_input": "2025-08-06T22:43:04.186684Z",
          "iopub.status.idle": "2025-08-07T01:25:46.445712Z",
          "shell.execute_reply.started": "2025-08-06T22:43:04.186651Z",
          "shell.execute_reply": "2025-08-07T01:25:46.444772Z"
        },
        "id": "xgYc9qOsDo9S",
        "outputId": "839df516-f6b1-4773-a351-ab859cb8c48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\nFound 855 videos total\nClass distribution: Counter({0: 531, 1: 324})\nData split - Train: 598, Val: 128, Test: 129\nClass weights: tensor([0.6119, 1.0000])\n\nStarting training for 50 epochs...\n============================================================\n\nEpoch 1/50\n------------------------------\n  Batch [0/150] Loss: 0.5415\n  Batch [10/150] Loss: 0.3419\n  Batch [20/150] Loss: 0.3943\n  Batch [30/150] Loss: 0.0199\n  Batch [40/150] Loss: 0.1183\n  Batch [50/150] Loss: 0.1068\n  Batch [60/150] Loss: 0.0390\n  Batch [70/150] Loss: 0.1535\n  Batch [80/150] Loss: 0.0949\n  Batch [90/150] Loss: 0.5750\n  Batch [100/150] Loss: 0.4312\n  Batch [110/150] Loss: 0.0478\n  Batch [120/150] Loss: 0.0034\n  Batch [130/150] Loss: 0.8165\n  Batch [140/150] Loss: 0.3257\nTrain Loss: 0.3237\nVal Loss: 0.2642\nVal Accuracy: 0.9453\nVal F1: 0.9419\nLearning Rate: 0.000100\n>>> New best model saved! F1: 0.9419\n\nEpoch 2/50\n------------------------------\n  Batch [0/150] Loss: 0.0218\n  Batch [10/150] Loss: 0.1321\n  Batch [20/150] Loss: 0.0061\n  Batch [30/150] Loss: 0.0177\n  Batch [40/150] Loss: 0.0056\n  Batch [50/150] Loss: 0.0061\n  Batch [60/150] Loss: 0.0097\n  Batch [70/150] Loss: 0.0032\n  Batch [80/150] Loss: 0.0027\n  Batch [90/150] Loss: 0.0235\n  Batch [100/150] Loss: 0.0023\n  Batch [110/150] Loss: 0.0010\n  Batch [120/150] Loss: 0.0006\n  Batch [130/150] Loss: 0.0020\n  Batch [140/150] Loss: 0.0007\nTrain Loss: 0.0969\nVal Loss: 2.1046\nVal Accuracy: 0.3750\nVal F1: 0.2727\nLearning Rate: 0.000100\n\nEpoch 3/50\n------------------------------\n  Batch [0/150] Loss: 0.6019\n  Batch [10/150] Loss: 0.0076\n  Batch [20/150] Loss: 1.6758\n  Batch [30/150] Loss: 2.8905\n  Batch [40/150] Loss: 0.3750\n  Batch [50/150] Loss: 0.0740\n  Batch [60/150] Loss: 0.0136\n  Batch [70/150] Loss: 0.0035\n  Batch [80/150] Loss: 0.2352\n  Batch [90/150] Loss: 0.0014\n  Batch [100/150] Loss: 0.0033\n  Batch [110/150] Loss: 0.0171\n  Batch [120/150] Loss: 0.0074\n  Batch [130/150] Loss: 0.0021\n  Batch [140/150] Loss: 0.0047\nTrain Loss: 0.1524\nVal Loss: 0.1406\nVal Accuracy: 0.9844\nVal F1: 0.9833\nLearning Rate: 0.000100\n>>> New best model saved! F1: 0.9833\n\nEpoch 4/50\n------------------------------\n  Batch [0/150] Loss: 0.0152\n  Batch [10/150] Loss: 0.0028\n  Batch [20/150] Loss: 0.0017\n  Batch [30/150] Loss: 0.0224\n  Batch [40/150] Loss: 0.0048\n  Batch [50/150] Loss: 0.0083\n  Batch [60/150] Loss: 0.0008\n  Batch [70/150] Loss: 0.0018\n  Batch [80/150] Loss: 0.0003\n  Batch [90/150] Loss: 0.0011\n  Batch [100/150] Loss: 0.0029\n  Batch [110/150] Loss: 0.0055\n  Batch [120/150] Loss: 0.0003\n  Batch [130/150] Loss: 0.2526\n  Batch [140/150] Loss: 0.0004\nTrain Loss: 0.0293\nVal Loss: 0.0938\nVal Accuracy: 0.9922\nVal F1: 0.9916\nLearning Rate: 0.000100\n>>> New best model saved! F1: 0.9916\n\nEpoch 5/50\n------------------------------\n  Batch [0/150] Loss: 0.0033\n  Batch [10/150] Loss: 0.0026\n  Batch [20/150] Loss: 0.0002\n  Batch [30/150] Loss: 0.0002\n  Batch [40/150] Loss: 0.0001\n  Batch [50/150] Loss: 0.0001\n  Batch [60/150] Loss: 0.0006\n  Batch [70/150] Loss: 0.0018\n  Batch [80/150] Loss: 0.0002\n  Batch [90/150] Loss: 0.0072\n  Batch [100/150] Loss: 0.0005\n  Batch [110/150] Loss: 0.0003\n  Batch [120/150] Loss: 0.0019\n  Batch [130/150] Loss: 0.0001\n  Batch [140/150] Loss: 0.0001\nTrain Loss: 0.0039\nVal Loss: 0.1157\nVal Accuracy: 0.9297\nVal F1: 0.9273\nLearning Rate: 0.000100\n\nEpoch 6/50\n------------------------------\n  Batch [0/150] Loss: 0.0004\n  Batch [10/150] Loss: 0.0001\n  Batch [20/150] Loss: 0.0003\n  Batch [30/150] Loss: 0.0002\n  Batch [40/150] Loss: 0.0010\n  Batch [50/150] Loss: 0.0013\n  Batch [60/150] Loss: 0.0001\n  Batch [70/150] Loss: 0.0001\n  Batch [80/150] Loss: 0.0005\n  Batch [90/150] Loss: 0.0001\n  Batch [100/150] Loss: 0.0000\n  Batch [110/150] Loss: 0.0000\n  Batch [120/150] Loss: 0.0001\n  Batch [130/150] Loss: 0.0001\n  Batch [140/150] Loss: 0.0000\nTrain Loss: 0.0034\nVal Loss: 0.3681\nVal Accuracy: 0.8438\nVal F1: 0.8419\nLearning Rate: 0.000100\n\nEpoch 7/50\n------------------------------\n  Batch [0/150] Loss: 0.0025\n  Batch [10/150] Loss: 0.0000\n  Batch [20/150] Loss: 0.0040\n  Batch [30/150] Loss: 0.0000\n  Batch [40/150] Loss: 0.0000\n  Batch [50/150] Loss: 0.0006\n  Batch [60/150] Loss: 0.0000\n  Batch [70/150] Loss: 0.0002\n  Batch [80/150] Loss: 0.0001\n  Batch [90/150] Loss: 3.5637\n  Batch [100/150] Loss: 0.2825\n  Batch [110/150] Loss: 0.0191\n  Batch [120/150] Loss: 0.5906\n  Batch [130/150] Loss: 0.0471\n  Batch [140/150] Loss: 0.0136\nTrain Loss: 0.1656\nVal Loss: 0.0252\nVal Accuracy: 0.9844\nVal F1: 0.9835\nLearning Rate: 0.000100\n\nEpoch 8/50\n------------------------------\n  Batch [0/150] Loss: 0.2752\n  Batch [10/150] Loss: 0.0039\n  Batch [20/150] Loss: 0.1104\n  Batch [30/150] Loss: 0.0041\n  Batch [40/150] Loss: 0.1551\n  Batch [50/150] Loss: 0.0144\n  Batch [60/150] Loss: 0.0242\n  Batch [70/150] Loss: 0.1671\n  Batch [80/150] Loss: 0.0178\n  Batch [90/150] Loss: 0.1410\n  Batch [100/150] Loss: 0.0129\n  Batch [110/150] Loss: 0.0053\n  Batch [120/150] Loss: 0.0023\n  Batch [130/150] Loss: 0.0050\n  Batch [140/150] Loss: 0.0273\nTrain Loss: 0.1745\nVal Loss: 0.1834\nVal Accuracy: 0.9375\nVal F1: 0.9352\nLearning Rate: 0.000100\n\nEpoch 9/50\n------------------------------\n  Batch [0/150] Loss: 0.0013\n  Batch [10/150] Loss: 0.0044\n  Batch [20/150] Loss: 0.7699\n  Batch [30/150] Loss: 0.0124\n  Batch [40/150] Loss: 0.0018\n  Batch [50/150] Loss: 0.0029\n  Batch [60/150] Loss: 0.0013\n  Batch [70/150] Loss: 0.0009\n  Batch [80/150] Loss: 0.0063\n  Batch [90/150] Loss: 0.0011\n  Batch [100/150] Loss: 0.0004\n  Batch [110/150] Loss: 0.0057\n  Batch [120/150] Loss: 0.0005\n  Batch [130/150] Loss: 0.0013\n  Batch [140/150] Loss: 0.0008\nTrain Loss: 0.0260\nVal Loss: 0.1164\nVal Accuracy: 0.9688\nVal F1: 0.9672\nLearning Rate: 0.000100\n\nEpoch 10/50\n------------------------------\n  Batch [0/150] Loss: 0.0015\n  Batch [10/150] Loss: 0.0049\n  Batch [20/150] Loss: 0.0160\n  Batch [30/150] Loss: 0.0003\n  Batch [40/150] Loss: 0.0001\n  Batch [50/150] Loss: 0.0001\n  Batch [60/150] Loss: 0.0265\n  Batch [70/150] Loss: 0.0224\n  Batch [80/150] Loss: 0.0029\n  Batch [90/150] Loss: 0.0041\n  Batch [100/150] Loss: 0.0012\n  Batch [110/150] Loss: 0.0082\n  Batch [120/150] Loss: 0.0028\n  Batch [130/150] Loss: 0.0016\n  Batch [140/150] Loss: 0.0027\nTrain Loss: 0.0629\nVal Loss: 0.5426\nVal Accuracy: 0.8047\nVal F1: 0.8037\nLearning Rate: 0.000100\n\nEpoch 11/50\n------------------------------\n  Batch [0/150] Loss: 0.0027\n  Batch [10/150] Loss: 0.3511\n  Batch [20/150] Loss: 0.0014\n  Batch [30/150] Loss: 0.0094\n  Batch [40/150] Loss: 0.0003\n  Batch [50/150] Loss: 0.0005\n  Batch [60/150] Loss: 0.0018\n  Batch [70/150] Loss: 0.0006\n  Batch [80/150] Loss: 0.0002\n  Batch [90/150] Loss: 0.0006\n  Batch [100/150] Loss: 0.0093\n  Batch [110/150] Loss: 0.0006\n  Batch [120/150] Loss: 0.0062\n  Batch [130/150] Loss: 0.0433\n  Batch [140/150] Loss: 0.0019\nTrain Loss: 0.0375\nVal Loss: 0.1387\nVal Accuracy: 0.9766\nVal F1: 0.9753\nLearning Rate: 0.000050\n\nEpoch 12/50\n------------------------------\n  Batch [0/150] Loss: 0.0023\n  Batch [10/150] Loss: 0.0056\n  Batch [20/150] Loss: 0.0006\n  Batch [30/150] Loss: 0.0001\n  Batch [40/150] Loss: 0.0019\n  Batch [50/150] Loss: 0.0004\n  Batch [60/150] Loss: 0.0002\n  Batch [70/150] Loss: 0.0014\n  Batch [80/150] Loss: 0.0222\n  Batch [90/150] Loss: 0.0039\n  Batch [100/150] Loss: 0.0029\n  Batch [110/150] Loss: 0.0002\n  Batch [120/150] Loss: 0.6821\n  Batch [130/150] Loss: 0.0014\n  Batch [140/150] Loss: 0.0004\nTrain Loss: 0.0149\nVal Loss: 0.3344\nVal Accuracy: 0.8281\nVal F1: 0.8271\nLearning Rate: 0.000050\n\nEpoch 13/50\n------------------------------\n  Batch [0/150] Loss: 0.0004\n  Batch [10/150] Loss: 0.0005\n  Batch [20/150] Loss: 0.0101\n  Batch [30/150] Loss: 0.0029\n  Batch [40/150] Loss: 0.0001\n  Batch [50/150] Loss: 0.0010\n  Batch [60/150] Loss: 0.0001\n  Batch [70/150] Loss: 0.0007\n  Batch [80/150] Loss: 0.0002\n  Batch [90/150] Loss: 0.0001\n  Batch [100/150] Loss: 0.0002\n  Batch [110/150] Loss: 0.0016\n  Batch [120/150] Loss: 0.0001\n  Batch [130/150] Loss: 0.0001\n  Batch [140/150] Loss: 0.0061\nTrain Loss: 0.0027\nVal Loss: 0.2642\nVal Accuracy: 0.8672\nVal F1: 0.8653\nLearning Rate: 0.000050\n\nEpoch 14/50\n------------------------------\n  Batch [0/150] Loss: 0.0002\n  Batch [10/150] Loss: 0.0002\n  Batch [20/150] Loss: 0.0001\n  Batch [30/150] Loss: 0.0012\n  Batch [40/150] Loss: 0.0002\n  Batch [50/150] Loss: 0.0003\n  Batch [60/150] Loss: 0.0012\n  Batch [70/150] Loss: 0.0000\n  Batch [80/150] Loss: 0.0021\n  Batch [90/150] Loss: 0.0013\n  Batch [100/150] Loss: 0.6433\n  Batch [110/150] Loss: 0.7077\n  Batch [120/150] Loss: 0.0019\n  Batch [130/150] Loss: 0.6550\n  Batch [140/150] Loss: 0.0051\nTrain Loss: 0.0272\nVal Loss: 0.2525\nVal Accuracy: 0.8672\nVal F1: 0.8653\nLearning Rate: 0.000050\nEarly stopping triggered after 14 epochs\n\n============================================================\nFINAL EVALUATION ON TEST SET\n============================================================\nTest Results:\n  Loss: 0.0674\n  Accuracy: 1.0000\n  Precision: 1.0000\n  Recall: 1.0000\n  F1: 1.0000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}