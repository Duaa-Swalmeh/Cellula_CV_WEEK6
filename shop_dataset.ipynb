{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6GKosec2j8-",
        "outputId": "187411e0-e6ee-438a-e38b-b159b62f11b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.7)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.5.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (25.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.0-py3-none-any.whl (981 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.1-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.15.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.8.0\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision torchaudio tqdm numpy opencv-python scikit-learn albumentations torchmetrics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srrBnB4oXC4x",
        "outputId": "fccb51df-2cef-409c-93a1-6c58b1f57b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/Shop DataSet\"\n"
      ],
      "metadata": {
        "id": "ljZyiXCSPlVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_path = \"/content/drive/MyDrive/Shop DataSet.zip\"\n",
        "extract_path = \"/content/shop_dataset\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ],
      "metadata": {
        "id": "YWk0849Y5VFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring structure and nomenclature"
      ],
      "metadata": {
        "id": "lmmuEkuJ3flK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "data_root = \"/content/shop_dataset/Shop DataSet\"\n",
        "\n",
        "\n",
        "classes = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))])\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "\n",
        "def count_videos_per_class(root):\n",
        "    counter = {}\n",
        "    for cls in classes:\n",
        "        cls_path = os.path.join(root, cls)\n",
        "        vids = [f for f in os.listdir(cls_path) if f.lower().endswith(('.mp4', '.avi'))]\n",
        "        counter[cls] = len(vids)\n",
        "    return counter\n",
        "\n",
        "print(count_videos_per_class(data_root))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkATVMBi3crG",
        "outputId": "e22b43ba-ee8c-4578-c835-288bc9420731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['non shop lifters', 'shop lifters']\n",
            "{'non shop lifters': 531, 'shop lifters': 324}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ": Preprocessing & Augmentation"
      ],
      "metadata": {
        "id": "_6Xcrwya3iFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from albumentations import Compose, Resize, HorizontalFlip, RandomBrightnessContrast, GaussNoise\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Example of preparing a spatial transform for each frame\n",
        "spatial_transform = Compose([\n",
        "    Resize(112, 112),\n",
        "    HorizontalFlip(p=0.5),\n",
        "    RandomBrightnessContrast(p=0.3),\n",
        "\n",
        "], p=1.0)\n"
      ],
      "metadata": {
        "id": "BIF4C7lg3iY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset class with sampling and length standardization"
      ],
      "metadata": {
        "id": "HZOZKCe63k1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TheftVideoDataset(Dataset):\n",
        "    def __init__(self, video_paths, labels, num_frames=16, transform=None):\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.num_frames = num_frames\n",
        "        self.transform = transform  # spatial transform per frame\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def _load_video(self, path):\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frames = []\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    def _sample_frames(self, frames):\n",
        "        total = len(frames)\n",
        "        if total >= self.num_frames:\n",
        "            indices = np.linspace(0, total - 1, self.num_frames, dtype=int)\n",
        "            sampled = [frames[i] for i in indices]\n",
        "        else:\n",
        "            pad = [frames[-1]] * (self.num_frames - total)\n",
        "            sampled = frames + pad\n",
        "        return sampled\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        frames = self._load_video(path)\n",
        "        frames = self._sample_frames(frames)  # list of HxWxC\n",
        "\n",
        "        processed = []\n",
        "        for f in frames:\n",
        "            if self.transform:\n",
        "                augmented = self.transform(image=f)\n",
        "                f_proc = augmented[\"image\"]  # tensor CxHxW if ToTensorV2 used\n",
        "            else:\n",
        "                f_resized = cv2.resize(f, (112, 112))\n",
        "                f_proc = torch.from_numpy(f_resized).permute(2,0,1).float() / 255.0\n",
        "            processed.append(f_proc)\n",
        "\n",
        "        video_tensor = torch.stack(processed)  # shape: (T, C, H, W)\n",
        "        video_tensor = video_tensor.permute(1,0,2,3)  # (C, T, H, W) for 3D CNN consistency\n",
        "        return video_tensor, torch.tensor(label, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "-fGCY7hr3mLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building models from scratch"
      ],
      "metadata": {
        "id": "cy7xWyYC3oBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from albumentations import Compose, Resize, HorizontalFlip, RandomBrightnessContrast\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "nNOxhqZEf9OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 3D CNN Simplified\n"
      ],
      "metadata": {
        "id": "My2uOACJ3oZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Simple3DCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv3d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(2),\n",
        "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(2),\n",
        "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool3d((1,1,1)),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # x: (B, C, T, H, W)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "        )\n",
        "        self.fc = nn.Linear(128, embed_dim)\n",
        "\n",
        "    def forward(self, x):  # x: (B, T, C, H, W)\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        x = self.conv(x)  # (B*T, 128, 1,1)\n",
        "        x = x.view(B * T, -1)  # (B*T, 128)\n",
        "        x = self.fc(x)  # (B*T, embed_dim)\n",
        "        x = x.view(B, T, -1)\n",
        "        return x  # (B, T, embed_dim)\n"
      ],
      "metadata": {
        "id": "uvzqks5n3q3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.CNN + RNN"
      ],
      "metadata": {
        "id": "U_47SeQj3tkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_RNN(nn.Module):\n",
        "    def __init__(self, hidden_dim=128, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.encoder = CNNEncoder(embed_dim=256)\n",
        "        self.lstm = nn.LSTM(input_size=256, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):  # x: (B, T, C, H, W)\n",
        "        seq = self.encoder(x)  # (B, T, 256)\n",
        "        out, _ = self.lstm(seq)  # (B, T, hidden_dim)\n",
        "        last = out[:, -1, :]  # (B, hidden_dim)\n",
        "        logits = self.classifier(last)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "e3GxOTGY3u6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET CLASS"
      ],
      "metadata": {
        "id": "9-UykXFI3yiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TheftVideoDataset(Dataset):\n",
        "    def __init__(self, video_paths, labels, num_frames=16, transform=None):\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.num_frames = num_frames\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def _load_video(self, path):\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frames = []\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    def _sample_frames(self, frames):\n",
        "        total = len(frames)\n",
        "        if total >= self.num_frames:\n",
        "            indices = np.linspace(0, total - 1, self.num_frames, dtype=int)\n",
        "            sampled = [frames[i] for i in indices]\n",
        "        else:\n",
        "            # Pad with last frame if video is too short\n",
        "            pad = [frames[-1]] * (self.num_frames - total)\n",
        "            sampled = frames + pad\n",
        "        return sampled\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        frames = self._load_video(path)\n",
        "\n",
        "        if len(frames) == 0:\n",
        "            raise RuntimeError(f\"Failed to load video: {path}\")\n",
        "\n",
        "        frames = self._sample_frames(frames)\n",
        "\n",
        "        processed = []\n",
        "        for frame in frames:\n",
        "            if self.transform:\n",
        "                augmented = self.transform(image=frame)\n",
        "                frame_tensor = augmented[\"image\"]  # (C,H,W) float 0-1\n",
        "            else:\n",
        "                frame_resized = cv2.resize(frame, (112, 112))\n",
        "                frame_tensor = torch.from_numpy(frame_resized).permute(2,0,1).float() / 255.0\n",
        "            processed.append(frame_tensor)\n",
        "\n",
        "        video_tensor = torch.stack(processed)  # (T, C, H, W)\n",
        "        video_tensor = video_tensor.permute(1,0,2,3)  # (C, T, H, W)\n",
        "\n",
        "        return video_tensor, torch.tensor(label, dtype=torch.long)"
      ],
      "metadata": {
        "id": "Br_sqjdY3yzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING & EVALUATION"
      ],
      "metadata": {
        "id": "ZLdlLyxO313N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.save_checkpoint(model)\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            if self.restore_best_weights:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        self.best_weights = model.state_dict().copy()\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = len(loader)\n",
        "\n",
        "    for batch_idx, (videos, labels) in enumerate(loader):\n",
        "        videos = videos.float().to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(videos)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'  Batch [{batch_idx}/{num_batches}] Loss: {loss.item():.4f}')\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize metrics\n",
        "    acc_metric = Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
        "    prec_metric = Precision(task=\"multiclass\", average='macro', num_classes=2).to(device)\n",
        "    recall_metric = Recall(task=\"multiclass\", average='macro', num_classes=2).to(device)\n",
        "    f1_metric = F1Score(task=\"multiclass\", average='macro', num_classes=2).to(device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for videos, labels in loader:\n",
        "            videos = videos.float().to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(videos)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Update metrics\n",
        "            acc_metric.update(preds, labels)\n",
        "            prec_metric.update(preds, labels)\n",
        "            recall_metric.update(preds, labels)\n",
        "            f1_metric.update(preds, labels)\n",
        "\n",
        "    return {\n",
        "        \"loss\": total_loss / len(loader),\n",
        "        \"accuracy\": acc_metric.compute().item(),\n",
        "        \"precision\": prec_metric.compute().item(),\n",
        "        \"recall\": recall_metric.compute().item(),\n",
        "        \"f1\": f1_metric.compute().item()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "P5SJmdXG33JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREPARATION"
      ],
      "metadata": {
        "id": "dQaCAckF34y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prepare_data(data_root, test_size=0.3, val_size=0.5, random_state=42):\n",
        "    class_names = [\"non shop lifters\", \"shop lifters\"]\n",
        "    label_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "\n",
        "    # Collect video paths and labels\n",
        "    video_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for cls in class_names:\n",
        "        cls_dir = os.path.join(data_root, cls)\n",
        "        if not os.path.isdir(cls_dir):\n",
        "            raise FileNotFoundError(f\"Directory not found: {cls_dir}\")\n",
        "\n",
        "        for video_file in glob.glob(os.path.join(cls_dir, \"*\")):\n",
        "            if video_file.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\")):\n",
        "                video_paths.append(video_file)\n",
        "                labels.append(label_map[cls])\n",
        "\n",
        "    print(f\"Found {len(video_paths)} videos total\")\n",
        "    print(f\"Class distribution: {Counter(labels)}\")\n",
        "\n",
        "    # Stratified split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        video_paths, labels, stratify=labels, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, stratify=temp_labels, test_size=val_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    print(f\"Data split - Train: {len(train_paths)}, Val: {len(val_paths)}, Test: {len(test_paths)}\")\n",
        "\n",
        "    return (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels)\n",
        "\n",
        "def create_transforms():\n",
        "    train_transform = Compose([\n",
        "        Resize(112, 112),\n",
        "        HorizontalFlip(p=0.5),\n",
        "        RandomBrightnessContrast(p=0.3),\n",
        "        ToTensorV2(),\n",
        "    ], p=1.0)\n",
        "\n",
        "    val_transform = Compose([\n",
        "        Resize(112, 112),\n",
        "        ToTensorV2(),\n",
        "    ], p=1.0)\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "def create_dataloaders(train_data, val_data, test_data, train_transform, val_transform,\n",
        "                       num_frames=16, batch_size=4, num_workers=2):\n",
        "\n",
        "    train_paths, train_labels = train_data\n",
        "    val_paths, val_labels = val_data\n",
        "    test_paths, test_labels = test_data\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TheftVideoDataset(train_paths, train_labels, num_frames, train_transform)\n",
        "    val_dataset = TheftVideoDataset(val_paths, val_labels, num_frames, val_transform)\n",
        "    test_dataset = TheftVideoDataset(test_paths, test_labels, num_frames, val_transform)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                             num_workers=num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                           num_workers=num_workers, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                            num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "603JL10B36Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " MAIN TRAINING FUNCTION"
      ],
      "metadata": {
        "id": "WcL8cQu437o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model(data_root, model_name=\"Simple3DCNN\", num_epochs=50, learning_rate=1e-4,\n",
        "                batch_size=4, num_frames=16, patience=7):\n",
        "\n",
        "    # Setup device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Prepare data\n",
        "    train_data, val_data, test_data = prepare_data(data_root)\n",
        "    train_transform, val_transform = create_transforms()\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(\n",
        "        train_data, val_data, test_data, train_transform, val_transform,\n",
        "        num_frames, batch_size\n",
        "    )\n",
        "\n",
        "    # Calculate class weights\n",
        "    train_paths, train_labels = train_data\n",
        "    counter = Counter(train_labels)\n",
        "    class_names = [\"non shop lifters\", \"shop lifters\"]\n",
        "    class_counts = [counter[i] for i in range(len(class_names))]\n",
        "    weights = torch.tensor([1.0 / c if c > 0 else 0.0 for c in class_counts])\n",
        "    weights = weights / weights.max()\n",
        "    print(f\"Class weights: {weights}\")\n",
        "\n",
        "    # Initialize model\n",
        "    if model_name == \"Simple3DCNN\":\n",
        "        model = Simple3DCNN(num_classes=2).to(device)\n",
        "    elif model_name == \"CNN_RNN\":\n",
        "        model = CNN_RNN(num_classes=2).to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "    # Setup training components\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
        "    early_stopping = EarlyStopping(patience=patience, min_delta=0.001)\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [], 'val_accuracy': [],\n",
        "        'val_precision': [], 'val_recall': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Training phase\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_metrics[\"loss\"])\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_metrics[\"loss\"])\n",
        "        history['val_accuracy'].append(val_metrics[\"accuracy\"])\n",
        "        history['val_precision'].append(val_metrics[\"precision\"])\n",
        "        history['val_recall'].append(val_metrics[\"recall\"])\n",
        "        history['val_f1'].append(val_metrics[\"f1\"])\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
        "        print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"Val F1: {val_metrics['f1']:.4f}\")\n",
        "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_metrics[\"f1\"] > best_f1:\n",
        "            best_f1 = val_metrics[\"f1\"]\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_f1': best_f1,\n",
        "                'history': history\n",
        "            }, \"best_model.pth\")\n",
        "            print(f\">>> New best model saved! F1: {best_f1:.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if early_stopping(val_metrics[\"loss\"], model):\n",
        "            print(f\"Early stopping triggered after {epoch} epochs\")\n",
        "            break\n",
        "\n",
        "    # Load best model for testing\n",
        "    checkpoint = torch.load(\"best_model.pth\")\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL EVALUATION ON TEST SET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test_metrics = evaluate(model, test_loader, criterion, device)\n",
        "    print(\"Test Results:\")\n",
        "    for key, value in test_metrics.items():\n",
        "        print(f\"  {key.capitalize()}: {value:.4f}\")\n",
        "\n",
        "    return model, history, test_metrics"
      ],
      "metadata": {
        "id": "Zp35On4PGodH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREDICTION FUNCTION"
      ],
      "metadata": {
        "id": "Y0Iisenlg1vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_video(model, video_path, transform, num_frames=16, device='cpu', class_names=None):\n",
        "    \"\"\"Predict the class of a single video\"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [\"non shop lifters\", \"shop lifters\"]\n",
        "\n",
        "    # Create temporary dataset with dummy label\n",
        "    dataset = TheftVideoDataset([video_path], [0], num_frames=num_frames, transform=transform)\n",
        "    video_tensor, _ = dataset[0]\n",
        "    video_tensor = video_tensor.unsqueeze(0).float().to(device)  # Add batch dimension\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(video_tensor)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        pred = torch.argmax(probs, dim=1).item()\n",
        "        confidence = probs.max().item()\n",
        "\n",
        "    return {\n",
        "        'prediction': class_names[pred],\n",
        "        'confidence': confidence,\n",
        "        'probabilities': probs.squeeze().cpu().tolist()\n",
        "    }"
      ],
      "metadata": {
        "id": "AB1SpLOQ39KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    DATA_ROOT = \"/content/shop_dataset/Shop DataSet/\"\n",
        "    MODEL_NAME = \"Simple3DCNN\"\n",
        "\n",
        "    # Training parameters\n",
        "    config = {\n",
        "        'num_epochs': 50,\n",
        "        'learning_rate': 1e-4,\n",
        "        'batch_size': 4,\n",
        "        'num_frames': 16,\n",
        "        'patience': 7\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Train the model\n",
        "        model, history, test_metrics = train_model(DATA_ROOT, MODEL_NAME, **config)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpTKHh1Wg-6h",
        "outputId": "b4260bda-1333-44f1-bb17-15a21d349424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Found 855 videos total\n",
            "Class distribution: Counter({0: 531, 1: 324})\n",
            "Data split - Train: 598, Val: 128, Test: 129\n",
            "Class weights: tensor([0.6119, 1.0000])\n",
            "\n",
            "Starting training for 50 epochs...\n",
            "============================================================\n",
            "\n",
            "Epoch 1/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 2.0989\n",
            "  Batch [10/150] Loss: 0.5318\n",
            "  Batch [20/150] Loss: 1.3801\n",
            "  Batch [30/150] Loss: 0.8697\n",
            "  Batch [40/150] Loss: 0.6320\n",
            "  Batch [50/150] Loss: 0.5508\n",
            "  Batch [60/150] Loss: 0.6574\n",
            "  Batch [70/150] Loss: 0.4865\n",
            "  Batch [80/150] Loss: 0.6590\n",
            "  Batch [90/150] Loss: 0.8160\n",
            "  Batch [100/150] Loss: 0.7934\n",
            "  Batch [110/150] Loss: 0.7064\n",
            "  Batch [120/150] Loss: 0.7077\n",
            "  Batch [130/150] Loss: 0.6750\n",
            "  Batch [140/150] Loss: 0.7279\n",
            "Train Loss: 1.0009\n",
            "Val Loss: 0.6942\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            ">>> New best model saved! F1: 0.2727\n",
            "\n",
            "Epoch 2/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6791\n",
            "  Batch [10/150] Loss: 0.6951\n",
            "  Batch [20/150] Loss: 0.6548\n",
            "  Batch [30/150] Loss: 0.7486\n",
            "  Batch [40/150] Loss: 0.6254\n",
            "  Batch [50/150] Loss: 0.6923\n",
            "  Batch [60/150] Loss: 0.6898\n",
            "  Batch [70/150] Loss: 0.6822\n",
            "  Batch [80/150] Loss: 0.6618\n",
            "  Batch [90/150] Loss: 0.6988\n",
            "  Batch [100/150] Loss: 0.6708\n",
            "  Batch [110/150] Loss: 0.6860\n",
            "  Batch [120/150] Loss: 0.6847\n",
            "  Batch [130/150] Loss: 0.7162\n",
            "  Batch [140/150] Loss: 0.6506\n",
            "Train Loss: 0.6963\n",
            "Val Loss: 0.6930\n",
            "Val Accuracy: 0.6250\n",
            "Val F1: 0.3846\n",
            "Learning Rate: 0.000100\n",
            ">>> New best model saved! F1: 0.3846\n",
            "\n",
            "Epoch 3/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6915\n",
            "  Batch [10/150] Loss: 0.6874\n",
            "  Batch [20/150] Loss: 0.7018\n",
            "  Batch [30/150] Loss: 0.6256\n",
            "  Batch [40/150] Loss: 0.6792\n",
            "  Batch [50/150] Loss: 0.7353\n",
            "  Batch [60/150] Loss: 0.6544\n",
            "  Batch [70/150] Loss: 0.6826\n",
            "  Batch [80/150] Loss: 0.7092\n",
            "  Batch [90/150] Loss: 0.7135\n",
            "  Batch [100/150] Loss: 0.7355\n",
            "  Batch [110/150] Loss: 0.6737\n",
            "  Batch [120/150] Loss: 0.7024\n",
            "  Batch [130/150] Loss: 0.6976\n",
            "  Batch [140/150] Loss: 0.6755\n",
            "Train Loss: 0.6976\n",
            "Val Loss: 0.6928\n",
            "Val Accuracy: 0.6250\n",
            "Val F1: 0.3846\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 4/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6962\n",
            "  Batch [10/150] Loss: 0.6905\n",
            "  Batch [20/150] Loss: 0.7155\n",
            "  Batch [30/150] Loss: 0.6823\n",
            "  Batch [40/150] Loss: 0.7103\n",
            "  Batch [50/150] Loss: 0.7319\n",
            "  Batch [60/150] Loss: 0.6915\n",
            "  Batch [70/150] Loss: 0.6855\n",
            "  Batch [80/150] Loss: 0.6097\n",
            "  Batch [90/150] Loss: 0.6479\n",
            "  Batch [100/150] Loss: 0.6878\n",
            "  Batch [110/150] Loss: 0.6896\n",
            "  Batch [120/150] Loss: 0.7306\n",
            "  Batch [130/150] Loss: 0.7509\n",
            "  Batch [140/150] Loss: 0.6876\n",
            "Train Loss: 0.6960\n",
            "Val Loss: 0.6932\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2823\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 5/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6967\n",
            "  Batch [10/150] Loss: 0.6997\n",
            "  Batch [20/150] Loss: 0.6878\n",
            "  Batch [30/150] Loss: 0.6920\n",
            "  Batch [40/150] Loss: 0.6867\n",
            "  Batch [50/150] Loss: 0.6924\n",
            "  Batch [60/150] Loss: 0.6997\n",
            "  Batch [70/150] Loss: 0.6865\n",
            "  Batch [80/150] Loss: 0.7607\n",
            "  Batch [90/150] Loss: 0.5913\n",
            "  Batch [100/150] Loss: 0.7923\n",
            "  Batch [110/150] Loss: 0.7118\n",
            "  Batch [120/150] Loss: 0.6744\n",
            "  Batch [130/150] Loss: 0.6864\n",
            "  Batch [140/150] Loss: 0.6690\n",
            "Train Loss: 0.6962\n",
            "Val Loss: 0.6917\n",
            "Val Accuracy: 0.6250\n",
            "Val F1: 0.3846\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 6/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6856\n",
            "  Batch [10/150] Loss: 0.6693\n",
            "  Batch [20/150] Loss: 0.7030\n",
            "  Batch [30/150] Loss: 0.6624\n",
            "  Batch [40/150] Loss: 0.6371\n",
            "  Batch [50/150] Loss: 0.7332\n",
            "  Batch [60/150] Loss: 0.7434\n",
            "  Batch [70/150] Loss: 0.6773\n",
            "  Batch [80/150] Loss: 0.6783\n",
            "  Batch [90/150] Loss: 0.6865\n",
            "  Batch [100/150] Loss: 0.6953\n",
            "  Batch [110/150] Loss: 0.7112\n",
            "  Batch [120/150] Loss: 0.6999\n",
            "  Batch [130/150] Loss: 0.6988\n",
            "  Batch [140/150] Loss: 0.6941\n",
            "Train Loss: 0.6981\n",
            "Val Loss: 0.6940\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 7/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.7037\n",
            "  Batch [10/150] Loss: 0.6962\n",
            "  Batch [20/150] Loss: 0.6927\n",
            "  Batch [30/150] Loss: 0.7062\n",
            "  Batch [40/150] Loss: 0.6932\n",
            "  Batch [50/150] Loss: 0.6790\n",
            "  Batch [60/150] Loss: 0.6737\n",
            "  Batch [70/150] Loss: 0.6787\n",
            "  Batch [80/150] Loss: 0.7004\n",
            "  Batch [90/150] Loss: 0.6420\n",
            "  Batch [100/150] Loss: 0.6871\n",
            "  Batch [110/150] Loss: 0.6449\n",
            "  Batch [120/150] Loss: 0.6554\n",
            "  Batch [130/150] Loss: 0.6993\n",
            "  Batch [140/150] Loss: 0.7246\n",
            "Train Loss: 0.6948\n",
            "Val Loss: 0.6928\n",
            "Val Accuracy: 0.6797\n",
            "Val F1: 0.5856\n",
            "Learning Rate: 0.000100\n",
            ">>> New best model saved! F1: 0.5856\n",
            "\n",
            "Epoch 8/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.7079\n",
            "  Batch [10/150] Loss: 0.6726\n",
            "  Batch [20/150] Loss: 0.7031\n",
            "  Batch [30/150] Loss: 0.7017\n",
            "  Batch [40/150] Loss: 0.6831\n",
            "  Batch [50/150] Loss: 0.6699\n",
            "  Batch [60/150] Loss: 0.6740\n",
            "  Batch [70/150] Loss: 0.6982\n",
            "  Batch [80/150] Loss: 0.7083\n",
            "  Batch [90/150] Loss: 0.6816\n",
            "  Batch [100/150] Loss: 0.6843\n",
            "  Batch [110/150] Loss: 0.6920\n",
            "  Batch [120/150] Loss: 0.6830\n",
            "  Batch [130/150] Loss: 0.6561\n",
            "  Batch [140/150] Loss: 0.7736\n",
            "Train Loss: 0.6946\n",
            "Val Loss: 0.6905\n",
            "Val Accuracy: 0.6250\n",
            "Val F1: 0.3846\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 9/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6669\n",
            "  Batch [10/150] Loss: 0.7152\n",
            "  Batch [20/150] Loss: 0.7556\n",
            "  Batch [30/150] Loss: 0.8257\n",
            "  Batch [40/150] Loss: 0.7097\n",
            "  Batch [50/150] Loss: 0.6888\n",
            "  Batch [60/150] Loss: 0.6840\n",
            "  Batch [70/150] Loss: 0.6888\n",
            "  Batch [80/150] Loss: 0.7263\n",
            "  Batch [90/150] Loss: 0.6860\n",
            "  Batch [100/150] Loss: 0.6735\n",
            "  Batch [110/150] Loss: 0.6498\n",
            "  Batch [120/150] Loss: 0.6996\n",
            "  Batch [130/150] Loss: 0.6812\n",
            "  Batch [140/150] Loss: 0.6792\n",
            "Train Loss: 0.6935\n",
            "Val Loss: 0.6979\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 10/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6805\n",
            "  Batch [10/150] Loss: 0.6717\n",
            "  Batch [20/150] Loss: 0.6825\n",
            "  Batch [30/150] Loss: 0.7173\n",
            "  Batch [40/150] Loss: 0.8277\n",
            "  Batch [50/150] Loss: 0.6878\n",
            "  Batch [60/150] Loss: 0.6472\n",
            "  Batch [70/150] Loss: 0.6585\n",
            "  Batch [80/150] Loss: 0.7214\n",
            "  Batch [90/150] Loss: 0.6063\n",
            "  Batch [100/150] Loss: 0.6436\n",
            "  Batch [110/150] Loss: 0.6597\n",
            "  Batch [120/150] Loss: 0.7405\n",
            "  Batch [130/150] Loss: 0.6677\n",
            "  Batch [140/150] Loss: 0.7270\n",
            "Train Loss: 0.6904\n",
            "Val Loss: 0.6935\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 11/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6863\n",
            "  Batch [10/150] Loss: 0.7004\n",
            "  Batch [20/150] Loss: 0.6732\n",
            "  Batch [30/150] Loss: 0.6473\n",
            "  Batch [40/150] Loss: 0.6694\n",
            "  Batch [50/150] Loss: 0.6717\n",
            "  Batch [60/150] Loss: 0.7297\n",
            "  Batch [70/150] Loss: 0.6737\n",
            "  Batch [80/150] Loss: 0.7568\n",
            "  Batch [90/150] Loss: 0.6569\n",
            "  Batch [100/150] Loss: 0.6307\n",
            "  Batch [110/150] Loss: 0.6410\n",
            "  Batch [120/150] Loss: 0.6415\n",
            "  Batch [130/150] Loss: 0.6631\n",
            "  Batch [140/150] Loss: 0.6567\n",
            "Train Loss: 0.6788\n",
            "Val Loss: 0.6961\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 12/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6607\n",
            "  Batch [10/150] Loss: 0.6890\n",
            "  Batch [20/150] Loss: 0.6304\n",
            "  Batch [30/150] Loss: 0.7493\n",
            "  Batch [40/150] Loss: 0.7081\n",
            "  Batch [50/150] Loss: 0.6729\n",
            "  Batch [60/150] Loss: 1.3108\n",
            "  Batch [70/150] Loss: 0.5957\n",
            "  Batch [80/150] Loss: 0.5611\n",
            "  Batch [90/150] Loss: 0.8157\n",
            "  Batch [100/150] Loss: 0.6484\n",
            "  Batch [110/150] Loss: 0.6463\n",
            "  Batch [120/150] Loss: 0.6462\n",
            "  Batch [130/150] Loss: 0.6545\n",
            "  Batch [140/150] Loss: 1.5081\n",
            "Train Loss: 0.6756\n",
            "Val Loss: 0.6838\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 13/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.7063\n",
            "  Batch [10/150] Loss: 1.1905\n",
            "  Batch [20/150] Loss: 0.7109\n",
            "  Batch [30/150] Loss: 0.5433\n",
            "  Batch [40/150] Loss: 0.5862\n",
            "  Batch [50/150] Loss: 0.5439\n",
            "  Batch [60/150] Loss: 0.5496\n",
            "  Batch [70/150] Loss: 0.7683\n",
            "  Batch [80/150] Loss: 0.6241\n",
            "  Batch [90/150] Loss: 0.5938\n",
            "  Batch [100/150] Loss: 0.6530\n",
            "  Batch [110/150] Loss: 0.6847\n",
            "  Batch [120/150] Loss: 0.6430\n",
            "  Batch [130/150] Loss: 0.5386\n",
            "  Batch [140/150] Loss: 0.5334\n",
            "Train Loss: 0.6308\n",
            "Val Loss: 0.7269\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 14/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6005\n",
            "  Batch [10/150] Loss: 0.5887\n",
            "  Batch [20/150] Loss: 0.3492\n",
            "  Batch [30/150] Loss: 0.6354\n",
            "  Batch [40/150] Loss: 0.6319\n",
            "  Batch [50/150] Loss: 0.5151\n",
            "  Batch [60/150] Loss: 0.6798\n",
            "  Batch [70/150] Loss: 0.7162\n",
            "  Batch [80/150] Loss: 0.3999\n",
            "  Batch [90/150] Loss: 0.6667\n",
            "  Batch [100/150] Loss: 0.3695\n",
            "  Batch [110/150] Loss: 0.2729\n",
            "  Batch [120/150] Loss: 1.2493\n",
            "  Batch [130/150] Loss: 0.2941\n",
            "  Batch [140/150] Loss: 0.5743\n",
            "Train Loss: 0.5368\n",
            "Val Loss: 0.6513\n",
            "Val Accuracy: 0.7734\n",
            "Val F1: 0.7703\n",
            "Learning Rate: 0.000100\n",
            ">>> New best model saved! F1: 0.7703\n",
            "\n",
            "Epoch 15/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.5402\n",
            "  Batch [10/150] Loss: 1.2610\n",
            "  Batch [20/150] Loss: 0.5011\n",
            "  Batch [30/150] Loss: 0.5014\n",
            "  Batch [40/150] Loss: 0.3588\n",
            "  Batch [50/150] Loss: 0.6117\n",
            "  Batch [60/150] Loss: 0.5954\n",
            "  Batch [70/150] Loss: 0.9177\n",
            "  Batch [80/150] Loss: 0.2092\n",
            "  Batch [90/150] Loss: 0.4323\n",
            "  Batch [100/150] Loss: 0.1686\n",
            "  Batch [110/150] Loss: 0.5269\n",
            "  Batch [120/150] Loss: 0.3091\n",
            "  Batch [130/150] Loss: 0.2730\n",
            "  Batch [140/150] Loss: 0.4382\n",
            "Train Loss: 0.5135\n",
            "Val Loss: 0.6223\n",
            "Val Accuracy: 0.7344\n",
            "Val F1: 0.6865\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 16/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.4956\n",
            "  Batch [10/150] Loss: 0.4619\n",
            "  Batch [20/150] Loss: 0.2590\n",
            "  Batch [30/150] Loss: 0.4615\n",
            "  Batch [40/150] Loss: 0.8395\n",
            "  Batch [50/150] Loss: 0.4257\n",
            "  Batch [60/150] Loss: 0.4408\n",
            "  Batch [70/150] Loss: 0.3612\n",
            "  Batch [80/150] Loss: 0.2980\n",
            "  Batch [90/150] Loss: 0.2424\n",
            "  Batch [100/150] Loss: 0.4795\n",
            "  Batch [110/150] Loss: 0.6371\n",
            "  Batch [120/150] Loss: 0.7140\n",
            "  Batch [130/150] Loss: 0.3757\n",
            "  Batch [140/150] Loss: 0.6346\n",
            "Train Loss: 0.4418\n",
            "Val Loss: 0.6331\n",
            "Val Accuracy: 0.6250\n",
            "Val F1: 0.6242\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 17/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.5300\n",
            "  Batch [10/150] Loss: 0.3482\n",
            "  Batch [20/150] Loss: 0.4710\n",
            "  Batch [30/150] Loss: 0.3710\n",
            "  Batch [40/150] Loss: 0.6173\n",
            "  Batch [50/150] Loss: 0.8310\n",
            "  Batch [60/150] Loss: 0.2741\n",
            "  Batch [70/150] Loss: 0.8828\n",
            "  Batch [80/150] Loss: 0.7879\n",
            "  Batch [90/150] Loss: 0.1691\n",
            "  Batch [100/150] Loss: 0.6543\n",
            "  Batch [110/150] Loss: 0.4237\n",
            "  Batch [120/150] Loss: 0.1861\n",
            "  Batch [130/150] Loss: 0.2261\n",
            "  Batch [140/150] Loss: 0.3751\n",
            "Train Loss: 0.4268\n",
            "Val Loss: 0.7411\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 18/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.1816\n",
            "  Batch [10/150] Loss: 0.5101\n",
            "  Batch [20/150] Loss: 1.0727\n",
            "  Batch [30/150] Loss: 0.2841\n",
            "  Batch [40/150] Loss: 0.3084\n",
            "  Batch [50/150] Loss: 0.7538\n",
            "  Batch [60/150] Loss: 0.5496\n",
            "  Batch [70/150] Loss: 0.4348\n",
            "  Batch [80/150] Loss: 0.2588\n",
            "  Batch [90/150] Loss: 0.6450\n",
            "  Batch [100/150] Loss: 0.7624\n",
            "  Batch [110/150] Loss: 0.4315\n",
            "  Batch [120/150] Loss: 0.4808\n",
            "  Batch [130/150] Loss: 0.1516\n",
            "  Batch [140/150] Loss: 0.1965\n",
            "Train Loss: 0.4199\n",
            "Val Loss: 0.8266\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 19/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.1310\n",
            "  Batch [10/150] Loss: 0.3281\n",
            "  Batch [20/150] Loss: 0.4676\n",
            "  Batch [30/150] Loss: 1.0008\n",
            "  Batch [40/150] Loss: 0.5294\n",
            "  Batch [50/150] Loss: 0.0805\n",
            "  Batch [60/150] Loss: 0.2523\n",
            "  Batch [70/150] Loss: 0.7059\n",
            "  Batch [80/150] Loss: 0.2857\n",
            "  Batch [90/150] Loss: 0.2524\n",
            "  Batch [100/150] Loss: 0.2279\n",
            "  Batch [110/150] Loss: 0.4067\n",
            "  Batch [120/150] Loss: 0.3959\n",
            "  Batch [130/150] Loss: 0.3513\n",
            "  Batch [140/150] Loss: 0.5801\n",
            "Train Loss: 0.3928\n",
            "Val Loss: 0.5470\n",
            "Val Accuracy: 0.7734\n",
            "Val F1: 0.7683\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 20/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.5575\n",
            "  Batch [10/150] Loss: 0.5293\n",
            "  Batch [20/150] Loss: 0.4714\n",
            "  Batch [30/150] Loss: 0.2430\n",
            "  Batch [40/150] Loss: 0.2667\n",
            "  Batch [50/150] Loss: 0.2318\n",
            "  Batch [60/150] Loss: 0.2414\n",
            "  Batch [70/150] Loss: 0.2824\n",
            "  Batch [80/150] Loss: 0.2326\n",
            "  Batch [90/150] Loss: 0.3493\n",
            "  Batch [100/150] Loss: 0.1705\n",
            "  Batch [110/150] Loss: 0.3420\n",
            "  Batch [120/150] Loss: 0.1306\n",
            "  Batch [130/150] Loss: 0.2485\n",
            "  Batch [140/150] Loss: 0.3211\n",
            "Train Loss: 0.3993\n",
            "Val Loss: 0.6889\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 21/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.1539\n",
            "  Batch [10/150] Loss: 0.3188\n",
            "  Batch [20/150] Loss: 0.4587\n",
            "  Batch [30/150] Loss: 0.6876\n",
            "  Batch [40/150] Loss: 0.1973\n",
            "  Batch [50/150] Loss: 0.2141\n",
            "  Batch [60/150] Loss: 0.6513\n",
            "  Batch [70/150] Loss: 0.2790\n",
            "  Batch [80/150] Loss: 0.4208\n",
            "  Batch [90/150] Loss: 0.7290\n",
            "  Batch [100/150] Loss: 0.1835\n",
            "  Batch [110/150] Loss: 0.8347\n",
            "  Batch [120/150] Loss: 0.3051\n",
            "  Batch [130/150] Loss: 0.3075\n",
            "  Batch [140/150] Loss: 0.1108\n",
            "Train Loss: 0.4009\n",
            "Val Loss: 0.6649\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 22/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.3087\n",
            "  Batch [10/150] Loss: 0.6044\n",
            "  Batch [20/150] Loss: 0.4437\n",
            "  Batch [30/150] Loss: 0.4342\n",
            "  Batch [40/150] Loss: 0.2032\n",
            "  Batch [50/150] Loss: 0.1740\n",
            "  Batch [60/150] Loss: 0.1427\n",
            "  Batch [70/150] Loss: 0.1995\n",
            "  Batch [80/150] Loss: 0.5971\n",
            "  Batch [90/150] Loss: 0.2802\n",
            "  Batch [100/150] Loss: 0.2653\n",
            "  Batch [110/150] Loss: 0.6788\n",
            "  Batch [120/150] Loss: 0.2252\n",
            "  Batch [130/150] Loss: 0.2414\n",
            "  Batch [140/150] Loss: 0.0894\n",
            "Train Loss: 0.3907\n",
            "Val Loss: 0.5533\n",
            "Val Accuracy: 0.6953\n",
            "Val F1: 0.6948\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 23/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.1809\n",
            "  Batch [10/150] Loss: 0.0890\n",
            "  Batch [20/150] Loss: 0.0448\n",
            "  Batch [30/150] Loss: 0.1091\n",
            "  Batch [40/150] Loss: 0.4022\n",
            "  Batch [50/150] Loss: 0.1596\n",
            "  Batch [60/150] Loss: 0.4748\n",
            "  Batch [70/150] Loss: 0.2914\n",
            "  Batch [80/150] Loss: 0.6779\n",
            "  Batch [90/150] Loss: 0.4659\n",
            "  Batch [100/150] Loss: 0.3527\n",
            "  Batch [110/150] Loss: 0.1530\n",
            "  Batch [120/150] Loss: 0.1888\n",
            "  Batch [130/150] Loss: 0.3117\n",
            "  Batch [140/150] Loss: 0.2382\n",
            "Train Loss: 0.3524\n",
            "Val Loss: 0.4504\n",
            "Val Accuracy: 0.8672\n",
            "Val F1: 0.8577\n",
            "Learning Rate: 0.000100\n",
            ">>> New best model saved! F1: 0.8577\n",
            "\n",
            "Epoch 24/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.3054\n",
            "  Batch [10/150] Loss: 0.1163\n",
            "  Batch [20/150] Loss: 0.8379\n",
            "  Batch [30/150] Loss: 0.2870\n",
            "  Batch [40/150] Loss: 0.3913\n",
            "  Batch [50/150] Loss: 0.2275\n",
            "  Batch [60/150] Loss: 0.7231\n",
            "  Batch [70/150] Loss: 1.2871\n",
            "  Batch [80/150] Loss: 0.2823\n",
            "  Batch [90/150] Loss: 0.4687\n",
            "  Batch [100/150] Loss: 0.5020\n",
            "  Batch [110/150] Loss: 0.5495\n",
            "  Batch [120/150] Loss: 0.1949\n",
            "  Batch [130/150] Loss: 0.1330\n",
            "  Batch [140/150] Loss: 0.1377\n",
            "Train Loss: 0.3530\n",
            "Val Loss: 0.4517\n",
            "Val Accuracy: 0.8672\n",
            "Val F1: 0.8577\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 25/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.4010\n",
            "  Batch [10/150] Loss: 0.3631\n",
            "  Batch [20/150] Loss: 0.2610\n",
            "  Batch [30/150] Loss: 0.2012\n",
            "  Batch [40/150] Loss: 0.3541\n",
            "  Batch [50/150] Loss: 0.0846\n",
            "  Batch [60/150] Loss: 0.1785\n",
            "  Batch [70/150] Loss: 0.1033\n",
            "  Batch [80/150] Loss: 0.1687\n",
            "  Batch [90/150] Loss: 1.0979\n",
            "  Batch [100/150] Loss: 0.3055\n",
            "  Batch [110/150] Loss: 0.0154\n",
            "  Batch [120/150] Loss: 0.4399\n",
            "  Batch [130/150] Loss: 0.4767\n",
            "  Batch [140/150] Loss: 0.1091\n",
            "Train Loss: 0.3455\n",
            "Val Loss: 0.7776\n",
            "Val Accuracy: 0.3750\n",
            "Val F1: 0.2727\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 26/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.9814\n",
            "  Batch [10/150] Loss: 0.1678\n",
            "  Batch [20/150] Loss: 0.3065\n",
            "  Batch [30/150] Loss: 0.1824\n",
            "  Batch [40/150] Loss: 0.0597\n",
            "  Batch [50/150] Loss: 0.3340\n",
            "  Batch [60/150] Loss: 0.0734\n",
            "  Batch [70/150] Loss: 0.1655\n",
            "  Batch [80/150] Loss: 0.4091\n",
            "  Batch [90/150] Loss: 0.4095\n",
            "  Batch [100/150] Loss: 0.3867\n",
            "  Batch [110/150] Loss: 0.6081\n",
            "  Batch [120/150] Loss: 0.4088\n",
            "  Batch [130/150] Loss: 0.3957\n",
            "  Batch [140/150] Loss: 0.8079\n",
            "Train Loss: 0.3518\n",
            "Val Loss: 0.4708\n",
            "Val Accuracy: 0.8438\n",
            "Val F1: 0.8347\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch 27/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6308\n",
            "  Batch [10/150] Loss: 0.0537\n",
            "  Batch [20/150] Loss: 0.3543\n",
            "  Batch [30/150] Loss: 0.8523\n",
            "  Batch [40/150] Loss: 0.2933\n",
            "  Batch [50/150] Loss: 0.6076\n",
            "  Batch [60/150] Loss: 0.4670\n",
            "  Batch [70/150] Loss: 0.0386\n",
            "  Batch [80/150] Loss: 0.4054\n",
            "  Batch [90/150] Loss: 0.6370\n",
            "  Batch [100/150] Loss: 0.2503\n",
            "  Batch [110/150] Loss: 0.7396\n",
            "  Batch [120/150] Loss: 0.0677\n",
            "  Batch [130/150] Loss: 0.1298\n",
            "  Batch [140/150] Loss: 0.1085\n",
            "Train Loss: 0.3289\n",
            "Val Loss: 0.5507\n",
            "Val Accuracy: 0.6797\n",
            "Val F1: 0.6797\n",
            "Learning Rate: 0.000050\n",
            "\n",
            "Epoch 28/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.3622\n",
            "  Batch [10/150] Loss: 0.4031\n",
            "  Batch [20/150] Loss: 0.1334\n",
            "  Batch [30/150] Loss: 0.5603\n",
            "  Batch [40/150] Loss: 1.2072\n",
            "  Batch [50/150] Loss: 0.2414\n",
            "  Batch [60/150] Loss: 0.6024\n",
            "  Batch [70/150] Loss: 1.0808\n",
            "  Batch [80/150] Loss: 0.2789\n",
            "  Batch [90/150] Loss: 0.2467\n",
            "  Batch [100/150] Loss: 0.2445\n",
            "  Batch [110/150] Loss: 0.3826\n",
            "  Batch [120/150] Loss: 0.1141\n",
            "  Batch [130/150] Loss: 0.3869\n",
            "  Batch [140/150] Loss: 0.4960\n",
            "Train Loss: 0.3399\n",
            "Val Loss: 0.5254\n",
            "Val Accuracy: 0.7109\n",
            "Val F1: 0.7108\n",
            "Learning Rate: 0.000050\n",
            "\n",
            "Epoch 29/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.2275\n",
            "  Batch [10/150] Loss: 0.3180\n",
            "  Batch [20/150] Loss: 0.3448\n",
            "  Batch [30/150] Loss: 0.0326\n",
            "  Batch [40/150] Loss: 0.1440\n",
            "  Batch [50/150] Loss: 0.3625\n",
            "  Batch [60/150] Loss: 0.2973\n",
            "  Batch [70/150] Loss: 0.1731\n",
            "  Batch [80/150] Loss: 0.0491\n",
            "  Batch [90/150] Loss: 0.5944\n",
            "  Batch [100/150] Loss: 0.7153\n",
            "  Batch [110/150] Loss: 0.6804\n",
            "  Batch [120/150] Loss: 0.3561\n",
            "  Batch [130/150] Loss: 0.1058\n",
            "  Batch [140/150] Loss: 0.1740\n",
            "Train Loss: 0.2979\n",
            "Val Loss: 0.4605\n",
            "Val Accuracy: 0.8125\n",
            "Val F1: 0.8068\n",
            "Learning Rate: 0.000050\n",
            "\n",
            "Epoch 30/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.3835\n",
            "  Batch [10/150] Loss: 0.0195\n",
            "  Batch [20/150] Loss: 0.4496\n",
            "  Batch [30/150] Loss: 0.7107\n",
            "  Batch [40/150] Loss: 0.2907\n",
            "  Batch [50/150] Loss: 0.1562\n",
            "  Batch [60/150] Loss: 0.2220\n",
            "  Batch [70/150] Loss: 0.2383\n",
            "  Batch [80/150] Loss: 0.2238\n",
            "  Batch [90/150] Loss: 0.1051\n",
            "  Batch [100/150] Loss: 0.1688\n",
            "  Batch [110/150] Loss: 0.0906\n",
            "  Batch [120/150] Loss: 0.0959\n",
            "  Batch [130/150] Loss: 0.0636\n",
            "  Batch [140/150] Loss: 0.1727\n",
            "Train Loss: 0.2750\n",
            "Val Loss: 0.3970\n",
            "Val Accuracy: 0.8672\n",
            "Val F1: 0.8577\n",
            "Learning Rate: 0.000050\n",
            "\n",
            "Epoch 31/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.2129\n",
            "  Batch [10/150] Loss: 0.0716\n",
            "  Batch [20/150] Loss: 0.2637\n",
            "  Batch [30/150] Loss: 0.2519\n",
            "  Batch [40/150] Loss: 0.0763\n",
            "  Batch [50/150] Loss: 0.0406\n",
            "  Batch [60/150] Loss: 0.2427\n",
            "  Batch [70/150] Loss: 0.1882\n",
            "  Batch [80/150] Loss: 0.0377\n",
            "  Batch [90/150] Loss: 0.3704\n",
            "  Batch [100/150] Loss: 0.8063\n",
            "  Batch [110/150] Loss: 0.2636\n",
            "  Batch [120/150] Loss: 0.0445\n",
            "  Batch [130/150] Loss: 0.3061\n",
            "  Batch [140/150] Loss: 0.2839\n",
            "Train Loss: 0.2924\n",
            "Val Loss: 0.4319\n",
            "Val Accuracy: 0.8359\n",
            "Val F1: 0.8314\n",
            "Learning Rate: 0.000050\n",
            "\n",
            "Epoch 32/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.8421\n",
            "  Batch [10/150] Loss: 0.1143\n",
            "  Batch [20/150] Loss: 0.1744\n",
            "  Batch [30/150] Loss: 0.3730\n",
            "  Batch [40/150] Loss: 0.0753\n",
            "  Batch [50/150] Loss: 0.1102\n",
            "  Batch [60/150] Loss: 0.0238\n",
            "  Batch [70/150] Loss: 0.5537\n",
            "  Batch [80/150] Loss: 0.4357\n",
            "  Batch [90/150] Loss: 0.5275\n",
            "  Batch [100/150] Loss: 0.2611\n",
            "  Batch [110/150] Loss: 0.1513\n",
            "  Batch [120/150] Loss: 0.1169\n",
            "  Batch [130/150] Loss: 0.2361\n",
            "  Batch [140/150] Loss: 0.3540\n",
            "Train Loss: 0.2936\n",
            "Val Loss: 0.4092\n",
            "Val Accuracy: 0.8672\n",
            "Val F1: 0.8600\n",
            "Learning Rate: 0.000050\n",
            ">>> New best model saved! F1: 0.8600\n",
            "\n",
            "Epoch 33/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.1795\n",
            "  Batch [10/150] Loss: 0.2997\n",
            "  Batch [20/150] Loss: 0.7256\n",
            "  Batch [30/150] Loss: 0.0992\n",
            "  Batch [40/150] Loss: 0.1224\n",
            "  Batch [50/150] Loss: 0.2171\n",
            "  Batch [60/150] Loss: 0.3391\n",
            "  Batch [70/150] Loss: 0.1272\n",
            "  Batch [80/150] Loss: 0.1727\n",
            "  Batch [90/150] Loss: 0.2144\n",
            "  Batch [100/150] Loss: 0.6278\n",
            "  Batch [110/150] Loss: 0.2776\n",
            "  Batch [120/150] Loss: 0.0857\n",
            "  Batch [130/150] Loss: 0.2370\n",
            "  Batch [140/150] Loss: 0.0491\n",
            "Train Loss: 0.2672\n",
            "Val Loss: 0.3858\n",
            "Val Accuracy: 0.8594\n",
            "Val F1: 0.8500\n",
            "Learning Rate: 0.000050\n",
            "\n",
            "Epoch 34/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.0410\n",
            "  Batch [10/150] Loss: 0.1056\n",
            "  Batch [20/150] Loss: 0.0532\n",
            "  Batch [30/150] Loss: 0.2987\n",
            "  Batch [40/150] Loss: 0.1272\n",
            "  Batch [50/150] Loss: 0.0532\n",
            "  Batch [60/150] Loss: 0.3034\n",
            "  Batch [70/150] Loss: 0.4708\n",
            "  Batch [80/150] Loss: 0.2683\n",
            "  Batch [90/150] Loss: 0.6475\n",
            "  Batch [100/150] Loss: 0.0763\n",
            "  Batch [110/150] Loss: 0.0672\n",
            "  Batch [120/150] Loss: 0.0520\n",
            "  Batch [130/150] Loss: 0.2560\n",
            "  Batch [140/150] Loss: 0.1487\n",
            "Train Loss: 0.2770\n",
            "Val Loss: 0.6149\n",
            "Val Accuracy: 0.6562\n",
            "Val F1: 0.6532\n",
            "Learning Rate: 0.000050\n",
            "\n",
            "Epoch 35/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.1225\n",
            "  Batch [10/150] Loss: 0.0679\n",
            "  Batch [20/150] Loss: 0.3925\n",
            "  Batch [30/150] Loss: 0.3173\n",
            "  Batch [40/150] Loss: 0.6832\n",
            "  Batch [50/150] Loss: 0.0549\n",
            "  Batch [60/150] Loss: 0.3370\n",
            "  Batch [70/150] Loss: 0.0768\n",
            "  Batch [80/150] Loss: 0.8156\n",
            "  Batch [90/150] Loss: 0.1507\n",
            "  Batch [100/150] Loss: 0.1683\n",
            "  Batch [110/150] Loss: 0.2479\n",
            "  Batch [120/150] Loss: 0.0703\n",
            "  Batch [130/150] Loss: 0.5139\n",
            "  Batch [140/150] Loss: 0.6342\n",
            "Train Loss: 0.2613\n",
            "Val Loss: 0.3701\n",
            "Val Accuracy: 0.8672\n",
            "Val F1: 0.8600\n",
            "Learning Rate: 0.000050\n",
            "\n",
            "Epoch 36/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.3552\n",
            "  Batch [10/150] Loss: 0.0850\n",
            "  Batch [20/150] Loss: 0.0488\n",
            "  Batch [30/150] Loss: 0.3802\n",
            "  Batch [40/150] Loss: 0.0619\n",
            "  Batch [50/150] Loss: 0.6127\n",
            "  Batch [60/150] Loss: 0.5301\n",
            "  Batch [70/150] Loss: 0.4401\n",
            "  Batch [80/150] Loss: 0.2575\n",
            "  Batch [90/150] Loss: 0.2352\n",
            "  Batch [100/150] Loss: 0.0782\n",
            "  Batch [110/150] Loss: 0.0254\n",
            "  Batch [120/150] Loss: 0.0126\n",
            "  Batch [130/150] Loss: 0.0847\n",
            "  Batch [140/150] Loss: 0.0590\n",
            "Train Loss: 0.2417\n",
            "Val Loss: 0.6666\n",
            "Val Accuracy: 0.5859\n",
            "Val F1: 0.5745\n",
            "Learning Rate: 0.000050\n",
            "\n",
            "Epoch 37/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.2955\n",
            "  Batch [10/150] Loss: 0.0320\n",
            "  Batch [20/150] Loss: 0.0447\n",
            "  Batch [30/150] Loss: 0.0282\n",
            "  Batch [40/150] Loss: 0.1912\n",
            "  Batch [50/150] Loss: 0.6125\n",
            "  Batch [60/150] Loss: 0.2578\n",
            "  Batch [70/150] Loss: 1.3912\n",
            "  Batch [80/150] Loss: 0.3655\n",
            "  Batch [90/150] Loss: 0.2717\n",
            "  Batch [100/150] Loss: 0.3522\n",
            "  Batch [110/150] Loss: 0.4213\n",
            "  Batch [120/150] Loss: 0.2301\n",
            "  Batch [130/150] Loss: 0.0050\n",
            "  Batch [140/150] Loss: 0.0925\n",
            "Train Loss: 0.2832\n",
            "Val Loss: 0.3974\n",
            "Val Accuracy: 0.8672\n",
            "Val F1: 0.8628\n",
            "Learning Rate: 0.000050\n",
            ">>> New best model saved! F1: 0.8628\n",
            "\n",
            "Epoch 38/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.1220\n",
            "  Batch [10/150] Loss: 0.3943\n",
            "  Batch [20/150] Loss: 0.2675\n",
            "  Batch [30/150] Loss: 0.0635\n",
            "  Batch [40/150] Loss: 0.0602\n",
            "  Batch [50/150] Loss: 0.0637\n",
            "  Batch [60/150] Loss: 0.1533\n",
            "  Batch [70/150] Loss: 0.3935\n",
            "  Batch [80/150] Loss: 0.1131\n",
            "  Batch [90/150] Loss: 0.0955\n",
            "  Batch [100/150] Loss: 0.2668\n",
            "  Batch [110/150] Loss: 0.3491\n",
            "  Batch [120/150] Loss: 0.1893\n",
            "  Batch [130/150] Loss: 0.4532\n",
            "  Batch [140/150] Loss: 0.5468\n",
            "Train Loss: 0.2442\n",
            "Val Loss: 0.8087\n",
            "Val Accuracy: 0.3828\n",
            "Val F1: 0.2866\n",
            "Learning Rate: 0.000050\n",
            "\n",
            "Epoch 39/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.4829\n",
            "  Batch [10/150] Loss: 0.4337\n",
            "  Batch [20/150] Loss: 0.1036\n",
            "  Batch [30/150] Loss: 0.0292\n",
            "  Batch [40/150] Loss: 0.3813\n",
            "  Batch [50/150] Loss: 0.1987\n",
            "  Batch [60/150] Loss: 0.3310\n",
            "  Batch [70/150] Loss: 0.1442\n",
            "  Batch [80/150] Loss: 0.1119\n",
            "  Batch [90/150] Loss: 0.1649\n",
            "  Batch [100/150] Loss: 0.2746\n",
            "  Batch [110/150] Loss: 0.0533\n",
            "  Batch [120/150] Loss: 0.0276\n",
            "  Batch [130/150] Loss: 0.4923\n",
            "  Batch [140/150] Loss: 0.2037\n",
            "Train Loss: 0.2213\n",
            "Val Loss: 0.5810\n",
            "Val Accuracy: 0.6719\n",
            "Val F1: 0.6699\n",
            "Learning Rate: 0.000025\n",
            "\n",
            "Epoch 40/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.1557\n",
            "  Batch [10/150] Loss: 0.1088\n",
            "  Batch [20/150] Loss: 0.1459\n",
            "  Batch [30/150] Loss: 1.2921\n",
            "  Batch [40/150] Loss: 1.3714\n",
            "  Batch [50/150] Loss: 0.3140\n",
            "  Batch [60/150] Loss: 0.2024\n",
            "  Batch [70/150] Loss: 0.0680\n",
            "  Batch [80/150] Loss: 0.4503\n",
            "  Batch [90/150] Loss: 0.1282\n",
            "  Batch [100/150] Loss: 0.0150\n",
            "  Batch [110/150] Loss: 0.0983\n",
            "  Batch [120/150] Loss: 0.2977\n",
            "  Batch [130/150] Loss: 0.0970\n",
            "  Batch [140/150] Loss: 0.0374\n",
            "Train Loss: 0.2224\n",
            "Val Loss: 0.5606\n",
            "Val Accuracy: 0.7031\n",
            "Val F1: 0.7025\n",
            "Learning Rate: 0.000025\n",
            "\n",
            "Epoch 41/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.2304\n",
            "  Batch [10/150] Loss: 0.0058\n",
            "  Batch [20/150] Loss: 0.1525\n",
            "  Batch [30/150] Loss: 0.3081\n",
            "  Batch [40/150] Loss: 0.1642\n",
            "  Batch [50/150] Loss: 0.1240\n",
            "  Batch [60/150] Loss: 0.1952\n",
            "  Batch [70/150] Loss: 0.0323\n",
            "  Batch [80/150] Loss: 0.1103\n",
            "  Batch [90/150] Loss: 0.1169\n",
            "  Batch [100/150] Loss: 0.1505\n",
            "  Batch [110/150] Loss: 0.2932\n",
            "  Batch [120/150] Loss: 0.0515\n",
            "  Batch [130/150] Loss: 0.0739\n",
            "  Batch [140/150] Loss: 0.0386\n",
            "Train Loss: 0.2025\n",
            "Val Loss: 0.7907\n",
            "Val Accuracy: 0.5078\n",
            "Val F1: 0.4771\n",
            "Learning Rate: 0.000025\n",
            "\n",
            "Epoch 42/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.5465\n",
            "  Batch [10/150] Loss: 0.0469\n",
            "  Batch [20/150] Loss: 0.0556\n",
            "  Batch [30/150] Loss: 0.1035\n",
            "  Batch [40/150] Loss: 0.0546\n",
            "  Batch [50/150] Loss: 0.0419\n",
            "  Batch [60/150] Loss: 0.0953\n",
            "  Batch [70/150] Loss: 0.0517\n",
            "  Batch [80/150] Loss: 0.2329\n",
            "  Batch [90/150] Loss: 0.1479\n",
            "  Batch [100/150] Loss: 0.1052\n",
            "  Batch [110/150] Loss: 0.2031\n",
            "  Batch [120/150] Loss: 0.3639\n",
            "  Batch [130/150] Loss: 0.1771\n",
            "  Batch [140/150] Loss: 0.1936\n",
            "Train Loss: 0.1922\n",
            "Val Loss: 0.3580\n",
            "Val Accuracy: 0.8906\n",
            "Val F1: 0.8859\n",
            "Learning Rate: 0.000025\n",
            ">>> New best model saved! F1: 0.8859\n",
            "\n",
            "Epoch 43/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.6899\n",
            "  Batch [10/150] Loss: 0.1639\n",
            "  Batch [20/150] Loss: 0.1281\n",
            "  Batch [30/150] Loss: 0.0915\n",
            "  Batch [40/150] Loss: 0.3148\n",
            "  Batch [50/150] Loss: 0.0230\n",
            "  Batch [60/150] Loss: 0.1802\n",
            "  Batch [70/150] Loss: 0.2169\n",
            "  Batch [80/150] Loss: 0.1418\n",
            "  Batch [90/150] Loss: 0.2085\n",
            "  Batch [100/150] Loss: 0.1703\n",
            "  Batch [110/150] Loss: 0.0309\n",
            "  Batch [120/150] Loss: 0.3459\n",
            "  Batch [130/150] Loss: 0.0143\n",
            "  Batch [140/150] Loss: 0.0713\n",
            "Train Loss: 0.2024\n",
            "Val Loss: 0.4530\n",
            "Val Accuracy: 0.7734\n",
            "Val F1: 0.7731\n",
            "Learning Rate: 0.000025\n",
            "\n",
            "Epoch 44/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.4675\n",
            "  Batch [10/150] Loss: 0.1087\n",
            "  Batch [20/150] Loss: 0.0386\n",
            "  Batch [30/150] Loss: 0.2044\n",
            "  Batch [40/150] Loss: 0.2750\n",
            "  Batch [50/150] Loss: 0.3874\n",
            "  Batch [60/150] Loss: 0.0776\n",
            "  Batch [70/150] Loss: 0.0464\n",
            "  Batch [80/150] Loss: 0.1174\n",
            "  Batch [90/150] Loss: 0.1801\n",
            "  Batch [100/150] Loss: 0.4905\n",
            "  Batch [110/150] Loss: 0.0314\n",
            "  Batch [120/150] Loss: 0.0233\n",
            "  Batch [130/150] Loss: 0.5329\n",
            "  Batch [140/150] Loss: 1.4031\n",
            "Train Loss: 0.2156\n",
            "Val Loss: 0.4033\n",
            "Val Accuracy: 0.8516\n",
            "Val F1: 0.8489\n",
            "Learning Rate: 0.000025\n",
            "\n",
            "Epoch 45/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.1893\n",
            "  Batch [10/150] Loss: 0.0198\n",
            "  Batch [20/150] Loss: 0.2981\n",
            "  Batch [30/150] Loss: 0.4053\n",
            "  Batch [40/150] Loss: 0.4585\n",
            "  Batch [50/150] Loss: 0.0800\n",
            "  Batch [60/150] Loss: 0.1136\n",
            "  Batch [70/150] Loss: 0.0459\n",
            "  Batch [80/150] Loss: 0.2488\n",
            "  Batch [90/150] Loss: 0.1540\n",
            "  Batch [100/150] Loss: 0.0202\n",
            "  Batch [110/150] Loss: 0.2812\n",
            "  Batch [120/150] Loss: 0.0378\n",
            "  Batch [130/150] Loss: 0.3906\n",
            "  Batch [140/150] Loss: 0.0653\n",
            "Train Loss: 0.1910\n",
            "Val Loss: 0.6514\n",
            "Val Accuracy: 0.6641\n",
            "Val F1: 0.6616\n",
            "Learning Rate: 0.000025\n",
            "\n",
            "Epoch 46/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.3323\n",
            "  Batch [10/150] Loss: 0.2773\n",
            "  Batch [20/150] Loss: 0.0752\n",
            "  Batch [30/150] Loss: 0.0606\n",
            "  Batch [40/150] Loss: 0.2854\n",
            "  Batch [50/150] Loss: 0.0994\n",
            "  Batch [60/150] Loss: 0.1498\n",
            "  Batch [70/150] Loss: 0.0136\n",
            "  Batch [80/150] Loss: 0.1576\n",
            "  Batch [90/150] Loss: 0.0937\n",
            "  Batch [100/150] Loss: 0.0301\n",
            "  Batch [110/150] Loss: 0.6043\n",
            "  Batch [120/150] Loss: 0.0423\n",
            "  Batch [130/150] Loss: 0.0961\n",
            "  Batch [140/150] Loss: 0.0404\n",
            "Train Loss: 0.1896\n",
            "Val Loss: 0.3469\n",
            "Val Accuracy: 0.8828\n",
            "Val F1: 0.8782\n",
            "Learning Rate: 0.000025\n",
            "\n",
            "Epoch 47/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.1147\n",
            "  Batch [10/150] Loss: 0.0556\n",
            "  Batch [20/150] Loss: 0.0302\n",
            "  Batch [30/150] Loss: 0.0166\n",
            "  Batch [40/150] Loss: 0.3934\n",
            "  Batch [50/150] Loss: 0.0395\n",
            "  Batch [60/150] Loss: 0.1036\n",
            "  Batch [70/150] Loss: 0.3766\n",
            "  Batch [80/150] Loss: 0.0289\n",
            "  Batch [90/150] Loss: 0.0657\n",
            "  Batch [100/150] Loss: 0.2393\n",
            "  Batch [110/150] Loss: 0.0089\n",
            "  Batch [120/150] Loss: 0.0393\n",
            "  Batch [130/150] Loss: 0.8567\n",
            "  Batch [140/150] Loss: 0.0079\n",
            "Train Loss: 0.1902\n",
            "Val Loss: 0.3742\n",
            "Val Accuracy: 0.8750\n",
            "Val F1: 0.8719\n",
            "Learning Rate: 0.000025\n",
            "\n",
            "Epoch 48/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.0332\n",
            "  Batch [10/150] Loss: 0.0680\n",
            "  Batch [20/150] Loss: 0.2690\n",
            "  Batch [30/150] Loss: 0.0630\n",
            "  Batch [40/150] Loss: 0.0358\n",
            "  Batch [50/150] Loss: 0.0327\n",
            "  Batch [60/150] Loss: 0.0113\n",
            "  Batch [70/150] Loss: 0.0597\n",
            "  Batch [80/150] Loss: 0.1450\n",
            "  Batch [90/150] Loss: 0.0099\n",
            "  Batch [100/150] Loss: 0.8912\n",
            "  Batch [110/150] Loss: 0.0408\n",
            "  Batch [120/150] Loss: 0.2477\n",
            "  Batch [130/150] Loss: 0.0723\n",
            "  Batch [140/150] Loss: 0.0325\n",
            "Train Loss: 0.1663\n",
            "Val Loss: 0.3114\n",
            "Val Accuracy: 0.8672\n",
            "Val F1: 0.8600\n",
            "Learning Rate: 0.000025\n",
            "\n",
            "Epoch 49/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.0584\n",
            "  Batch [10/150] Loss: 0.1433\n",
            "  Batch [20/150] Loss: 0.0409\n",
            "  Batch [30/150] Loss: 0.0256\n",
            "  Batch [40/150] Loss: 0.1378\n",
            "  Batch [50/150] Loss: 0.0075\n",
            "  Batch [60/150] Loss: 0.3477\n",
            "  Batch [70/150] Loss: 0.0050\n",
            "  Batch [80/150] Loss: 0.1102\n",
            "  Batch [90/150] Loss: 0.3160\n",
            "  Batch [100/150] Loss: 0.0745\n",
            "  Batch [110/150] Loss: 0.0028\n",
            "  Batch [120/150] Loss: 0.6869\n",
            "  Batch [130/150] Loss: 0.0742\n",
            "  Batch [140/150] Loss: 0.3770\n",
            "Train Loss: 0.1607\n",
            "Val Loss: 0.3729\n",
            "Val Accuracy: 0.8750\n",
            "Val F1: 0.8719\n",
            "Learning Rate: 0.000025\n",
            "\n",
            "Epoch 50/50\n",
            "------------------------------\n",
            "  Batch [0/150] Loss: 0.0166\n",
            "  Batch [10/150] Loss: 0.0196\n",
            "  Batch [20/150] Loss: 0.1006\n",
            "  Batch [30/150] Loss: 0.0512\n",
            "  Batch [40/150] Loss: 0.0153\n",
            "  Batch [50/150] Loss: 0.7566\n",
            "  Batch [60/150] Loss: 0.0315\n",
            "  Batch [70/150] Loss: 0.0389\n",
            "  Batch [80/150] Loss: 0.0690\n",
            "  Batch [90/150] Loss: 0.0864\n",
            "  Batch [100/150] Loss: 0.0588\n",
            "  Batch [110/150] Loss: 0.3427\n",
            "  Batch [120/150] Loss: 0.0062\n",
            "  Batch [130/150] Loss: 0.2219\n",
            "  Batch [140/150] Loss: 0.0173\n",
            "Train Loss: 0.1532\n",
            "Val Loss: 0.3888\n",
            "Val Accuracy: 0.8828\n",
            "Val F1: 0.8802\n",
            "Learning Rate: 0.000025\n",
            "\n",
            "============================================================\n",
            "FINAL EVALUATION ON TEST SET\n",
            "============================================================\n",
            "Test Results:\n",
            "  Loss: 0.3861\n",
            "  Accuracy: 0.8605\n",
            "  Precision: 0.8528\n",
            "  Recall: 0.8717\n",
            "  F1: 0.8567\n"
          ]
        }
      ]
    }
  ]
}